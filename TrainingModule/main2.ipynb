{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installazione delle Librerie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy pandas scikit-learn torch pytorch-tabnet pytorch-tabular joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import e definizioni delle funzioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "from pytorch_tabular import TabularModel\n",
    "from pytorch_tabular.config import DataConfig, ModelConfig, OptimizerConfig, TrainerConfig\n",
    "import joblib\n",
    "\n",
    "random_state = 89\n",
    "\n",
    "def save_model(model, filename):\n",
    "    joblib.dump(model, f'{filename}.pkl')\n",
    "\n",
    "def preprocess(X, scaler_type='standard', use_pca=False, n_components=None):\n",
    "    if scaler_type == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "    elif scaler_type == 'minmax':\n",
    "        scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    pca = None\n",
    "    if use_pca and n_components:\n",
    "        pca = PCA(n_components=n_components, random_state=random_state)\n",
    "        X_scaled = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    return X_scaled, scaler, pca\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caricamento dei Dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caricamento dei dati\n",
    "df = pd.read_csv('training_set.csv')\n",
    "X = df.drop(columns='Anno')\n",
    "y = df['Anno']\n",
    "\n",
    "# Suddividi i dati in training e validation set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funzioni di Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per addestrare i modelli tradizionali\n",
    "def train_model(X_train, y_train, model_type='LR'):\n",
    "    if model_type == 'LR':\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train, y_train)\n",
    "        best_params = None\n",
    "    elif model_type == 'RF':\n",
    "        model = RandomForestRegressor(random_state=random_state)\n",
    "        model.fit(X_train, y_train)\n",
    "        best_params = None\n",
    "    elif model_type == 'KNR':\n",
    "        model = KNeighborsRegressor()\n",
    "        model.fit(X_train, y_train)\n",
    "        best_params = None\n",
    "    elif model_type == 'SVR':\n",
    "        model = SVR()\n",
    "        model.fit(X_train, y_train)\n",
    "        best_params = None\n",
    "    return model, best_params\n",
    "\n",
    "# Funzione per addestrare la rete neurale feed-forward\n",
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(FeedForwardNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 64)\n",
    "        self.layer2 = nn.Linear(64, 64)\n",
    "        self.layer3 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "\n",
    "def train_ffnn(X_train, y_train, input_dim):\n",
    "    model = FeedForwardNN(input_dim)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "    \n",
    "    for epoch in range(100):  # Number of epochs can be adjusted\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train_tensor)\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Funzione per addestrare TabNet\n",
    "def train_tabnet(X_train, y_train, X_val, y_val):\n",
    "    tabnet = TabNetRegressor(verbose=1, seed=random_state)\n",
    "    tabnet.fit(X_train, y_train, eval_set=[(X_val, y_val)], patience=50, max_epochs=1000)\n",
    "    return tabnet\n",
    "\n",
    "# Funzione per addestrare TabTransformer\n",
    "def train_tabtransformer(df_train, df_val, target_col='Anno'):\n",
    "    data_config = DataConfig(\n",
    "        target=target_col,\n",
    "        continuous_cols=df_train.columns.drop(target_col).tolist()\n",
    "    )\n",
    "    model_config = ModelConfig(\n",
    "        task=\"regression\",\n",
    "        learning_rate=1e-3,\n",
    "        seed=random_state\n",
    "    )\n",
    "    trainer_config = TrainerConfig(\n",
    "        max_epochs=100,\n",
    "        gpus=1 if torch.cuda.is_available() else 0\n",
    "    )\n",
    "    tabular_model = TabularModel(\n",
    "        data_config=data_config,\n",
    "        model_config=model_config,\n",
    "        optimizer_config=OptimizerConfig(),\n",
    "        trainer_config=trainer_config\n",
    "    )\n",
    "    tabular_model.fit(train=df_train, validation=df_val)\n",
    "    return tabular_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing e salvataggio dei risultati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esegui il preprocessing specifico per ogni modello con Standard Scaler e Min-Max Scaler\n",
    "preprocessing_options = {\n",
    "    'LR': {'scaler_type': ['standard', 'minmax'], 'use_pca': [True, False], 'n_components': 52},\n",
    "    'RF': {'scaler_type': ['standard', 'minmax'], 'use_pca': [True, False], 'n_components': 52},\n",
    "    'KNR': {'scaler_type': ['standard', 'minmax'], 'use_pca': [True, False], 'n_components': 52},\n",
    "    'SVR': {'scaler_type': ['standard', 'minmax'], 'use_pca': [True, False], 'n_components': 52},\n",
    "    'FFNN': {'scaler_type': ['standard', 'minmax'], 'use_pca': [True, False], 'n_components': 52},\n",
    "    'TabNet': {'scaler_type': ['standard', 'minmax'], 'use_pca': [False], 'n_components': None},\n",
    "    'TabTransformer': {'scaler_type': ['standard', 'minmax'], 'use_pca': [False], 'n_components': None},\n",
    "}\n",
    "\n",
    "# Funzione per eseguire il preprocessing e salvare i risultati\n",
    "def preprocess_and_save(X_train, X_val, preprocessing_options):\n",
    "    scalers = {}\n",
    "    pcas = {}\n",
    "    for clfName, options in preprocessing_options.items():\n",
    "        for scaler_type in options['scaler_type']:\n",
    "            for use_pca in options['use_pca']:\n",
    "                key = f\"{clfName}_{scaler_type}_{'PCA' if use_pca else 'NoPCA'}\"\n",
    "                X_train_scaled, scaler, pca = preprocess(X_train, scaler_type=scaler_type, use_pca=use_pca, n_components=options['n_components'])\n",
    "                X_val_scaled = scaler.transform(X_val)\n",
    "                if pca:\n",
    "                    X_val_scaled = pca.transform(X_val_scaled)\n",
    "                scalers[key] = scaler\n",
    "                pcas[key] = pca\n",
    "                save_model(scaler, f'scaler_{key.lower()}')\n",
    "                if pca:\n",
    "                    save_model(pca, f'pca_{key.lower()}')\n",
    "    return scalers, pcas\n",
    "\n",
    "# Esegui il preprocessing per tutte le combinazioni di scaler e PCA\n",
    "scalers, pcas = preprocess_and_save(X_train, X_val, preprocessing_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training modelli tradizionali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addestra i modelli tradizionali con tutte le combinazioni di scaler e PCA\n",
    "models = {}\n",
    "for clfName in ['LR', 'RF', 'KNR', 'SVR']:\n",
    "    for scaler_type in preprocessing_options[clfName]['scaler_type']:\n",
    "        for use_pca in preprocessing_options[clfName]['use_pca']:\n",
    "            key = f\"{clfName}_{scaler_type}_{'PCA' if use_pca else 'NoPCA'}\"\n",
    "            X_train_scaled = scalers[key].transform(X_train)\n",
    "            if pcas[key]:\n",
    "                X_train_scaled = pcas[key].transform(X_train_scaled)\n",
    "            model, params = train_model(X_train_scaled, y_train, model_type=clfName)\n",
    "            models[key] = model\n",
    "            save_model(model, f'model_{clfName.lower()}_{scaler_type}_{\"pca\" if use_pca else \"nopca\"}')\n",
    "            print(f\"Trained {clfName} with {scaler_type} scaler and PCA={use_pca}, parameters: {params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training FFNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addestra il modello FFNN con tutte le combinazioni di scaler e PCA\n",
    "for scaler_type in preprocessing_options['FFNN']['scaler_type']:\n",
    "    for use_pca in preprocessing_options['FFNN']['use_pca']:\n",
    "        key = f\"FFNN_{scaler_type}_{'PCA' if use_pca else 'NoPCA'}\"\n",
    "        X_train_scaled = scalers[key].transform(X_train)\n",
    "        if pcas[key]:\n",
    "            X_train_scaled = pcas[key].transform(X_train_scaled)\n",
    "        ffnn_model = train_ffnn(X_train_scaled, y_train, input_dim=X_train_scaled.shape[1])\n",
    "        models[key] = ffnn_model\n",
    "        save_model(ffnn_model, f'model_ffnn_{scaler_type}_{\"pca\" if use_pca else \"nopca\"}')\n",
    "        print(f\"Trained FFNN with {scaler_type} scaler and PCA={use_pca}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training TabNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addestra il modello TabNet (solo senza PCA)\n",
    "for scaler_type in preprocessing_options['TabNet']['scaler_type']:\n",
    "    key = f\"TabNet_{scaler_type}_NoPCA\"\n",
    "    X_train_scaled = scalers[key].transform(X_train)\n",
    "    X_val_scaled = scalers[key].transform(X_val)\n",
    "    tabnet_model = train_tabnet(X_train_scaled, y_train, X_val_scaled, y_val)\n",
    "    models[key] = tabnet_model\n",
    "    save_model(tabnet_model, f'model_tabnet_{scaler_type}')\n",
    "    print(f\"Trained TabNet with {scaler_type} scaler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training TabTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addestra il modello TabTransformer (solo senza PCA)\n",
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "df_val = pd.concat([X_val, y_val], axis=1)\n",
    "\n",
    "for scaler_type in preprocessing_options['TabTransformer']['scaler_type']:\n",
    "    key = f\"TabTransformer_{scaler_type}_NoPCA\"\n",
    "    tabtransformer_model = train_tabtransformer(df_train, df_val, target_col='Anno')\n",
    "    models[key] = tabtransformer_model\n",
    "    save_model(tabtransformer_model, f'model_tabtransformer_{scaler_type}')\n",
    "    print(f\"Trained TabTransformer with {scaler_type} scaler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valutazione dei modelli con tutte le combinazioni di scaler e PCA\n",
    "performance = {}\n",
    "\n",
    "# Valutazione modelli tradizionali\n",
    "for clfName in ['LR', 'RF', 'KNR', 'SVR']:\n",
    "    for scaler_type in preprocessing_options[clfName]['scaler_type']:\n",
    "        for use_pca in preprocessing_options[clfName]['use_pca']:\n",
    "            key = f\"{clfName}_{scaler_type}_{'PCA' if use_pca else 'NoPCA'}\"\n",
    "            X_val_scaled = scalers[key].transform(X_val)\n",
    "            if pcas[key]:\n",
    "                X_val_scaled = pcas[key].transform(X_val_scaled)\n",
    "            y_pred = models[key].predict(X_val_scaled)\n",
    "            mse = mean_squared_error(y_val, y_pred)\n",
    "            mae = mean_absolute_error(y_val, y_pred)\n",
    "            r2 = r2_score(y_val, y_pred)\n",
    "            performance[key] = {'mse': mse, 'mae': mae, 'r2': r2}\n",
    "            print(f\"Performance of {clfName} with {scaler_type} scaler and PCA={use_pca}: MSE={mse}, MAE={mae}, R2={r2}\")\n",
    "\n",
    "# Valutazione FFNN\n",
    "for scaler_type in preprocessing_options['FFNN']['scaler_type']:\n",
    "    for use_pca in preprocessing_options['FFNN']['use_pca']:\n",
    "        key = f\"FFNN_{scaler_type}_{'PCA' if use_pca else 'NoPCA'}\"\n",
    "        X_val_scaled = scalers[key].transform(X_val)\n",
    "        if pcas[key]:\n",
    "            X_val_scaled = pcas[key].transform(X_val_scaled)\n",
    "        X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32)\n",
    "        y_pred_tensor = models[key](X_val_tensor).detach().numpy().squeeze()\n",
    "        mse = mean_squared_error(y_val, y_pred_tensor)\n",
    "        mae = mean_absolute_error(y_val, y_pred_tensor)\n",
    "        r2 = r2_score(y_val, y_pred_tensor)\n",
    "        performance[key] = {'mse': mse, 'mae': mae, 'r2': r2}\n",
    "        print(f\"Performance of FFNN with {scaler_type} scaler and PCA={use_pca}: MSE={mse}, MAE={mae}, R2={r2}\")\n",
    "\n",
    "# Valutazione TabNet\n",
    "for scaler_type in preprocessing_options['TabNet']['scaler_type']:\n",
    "    key = f\"TabNet_{scaler_type}_NoPCA\"\n",
    "    X_val_scaled = scalers[key].transform(X_val)\n",
    "    y_pred = models[key].predict(X_val_scaled)\n",
    "    mse = mean_squared_error(y_val, y_pred)\n",
    "    mae = mean_absolute_error(y_val, y_pred)\n",
    "    r2 = r2_score(y_val, y_pred)\n",
    "    performance[key] = {'mse': mse, 'mae': mae, 'r2': r2}\n",
    "    print(f\"Performance of TabNet with {scaler_type} scaler: MSE={mse}, MAE={mae}, R2={r2}\")\n",
    "\n",
    "# Valutazione TabTransformer\n",
    "for scaler_type in preprocessing_options['TabTransformer']['scaler_type']:\n",
    "    key = f\"TabTransformer_{scaler_type}_NoPCA\"\n",
    "    y_pred = models[key].predict(df_val).squeeze()\n",
    "    mse = mean_squared_error(y_val, y_pred)\n",
    "    mae = mean_absolute_error(y_val, y_pred)\n",
    "    r2 = r2_score(y_val, y_pred)\n",
    "    performance[key] = {'mse': mse, 'mae': mae, 'r2': r2}\n",
    "    print(f\"Performance of TabTransformer with {scaler_type} scaler: MSE={mse}, MAE={mae}, R2={r2}\")\n",
    "\n",
    "# Salva le performance dei modelli\n",
    "with open('performance.txt', 'w') as f:\n",
    "    for clfName, metrics in performance.items():\n",
    "        f.write(f\"{clfName}: MSE={metrics['mse']}, MAE={metrics['mae']}, R2={metrics['r2']}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
