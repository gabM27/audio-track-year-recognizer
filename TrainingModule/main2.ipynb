{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installazione delle Librerie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\gabriele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\gabriele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\gabriele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.5.0)\n",
      "Requirement already satisfied: torch in c:\\users\\gabriele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.3.0)\n",
      "Requirement already satisfied: pytorch-tabnet in c:\\users\\gabriele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.1.0)\n",
      "Requirement already satisfied: pytorch-tabular in c:\\users\\gabriele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\gabriele\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\gabriele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\gabriele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\gabriele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\gabriele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\gabriele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\gabriele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\gabriele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\gabriele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\gabriele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\gabriele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\gabriele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2024.5.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\gabriele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2021.4.0)\n",
      "Requirement already satisfied: tqdm>=4.36 in c:\\users\\gabriele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytorch-tabnet) (4.66.4)\n",
      "Requirement already satisfied: pytorch-lightning<2.2.0,>=2.0.0 in c:\\users\\gabriele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytorch-tabular) (2.1.4)\n",
      "Requirement already satisfied: omegaconf>=2.3.0 in c:\\users\\gabriele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytorch-tabular) (2.3.0)\n",
      "Requirement already satisfied: torchmetrics<1.3.0,>=0.10.0 in c:\\users\\gabriele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytorch-tabular) (1.2.1)\n",
      "Requirement already satisfied: tensorboard!=2.5.0,>2.2.0 in c:\\users\\gabriele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytorch-tabular) (2.16.2)\n",
      "Requirement already satisfied: protobuf<4.26.0,>=3.20.0 in c:\\users\\gabriele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytorch-tabular) (4.25.3)\n",
      "Requirement already satisfied: PyYAML<6.1.0,>=5.4 in c:\\users\\gabriele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytorch-tabular) (6.0.1)\n",
      "Requirement already satisfied: matplotlib>3.1 in c:\\users\\gabriele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytorch-tabular) (3.9.0)\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\gabriele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytorch-tabular) (8.1.2)\n",
      "Requirement already satisfied: einops<0.8.0,>=0.6.0 in c:\\users\\gabriele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytorch-tabular) (0.7.0)\n",
      "Requirement already satisfied: rich>=11.0.0 in c:\\users\\gabriele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytorch-tabular) (13.7.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\gabriele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>3.1->pytorch-tabular) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\gabriele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>3.1->pytorch-tabular) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\gabriele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>3.1->pytorch-tabular) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\gabriele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>3.1->pytorch-tabular) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\gabriele\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib>3.1->pytorch-tabular) (24.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\gabriele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>3.1->pytorch-tabular) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\gabriele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>3.1->pytorch-tabular) (3.1.2)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\gabriele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\gabriele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.12.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in c:\\users\\gabriele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from omegaconf>=2.3.0->pytorch-tabular) (4.9.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\gabriele\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in c:\\users\\gabriele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytorch-lightning<2.2.0,>=2.0.0->pytorch-tabular) (0.11.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\gabriele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich>=11.0.0->pytorch-tabular) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\gabriele\\appdata\\roaming\\python\\python312\\site-packages (from rich>=11.0.0->pytorch-tabular) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\gabriele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard!=2.5.0,>2.2.0->pytorch-tabular) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in c:\\users\\gabriele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard!=2.5.0,>2.2.0->pytorch-tabular) (1.64.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\gabriele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard!=2.5.0,>2.2.0->pytorch-tabular) (3.6)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\gabriele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard!=2.5.0,>2.2.0->pytorch-tabular) (70.0.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\gabriele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard!=2.5.0,>2.2.0->pytorch-tabular) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\gabriele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard!=2.5.0,>2.2.0->pytorch-tabular) (3.0.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\gabriele\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.36->pytorch-tabnet) (0.4.6)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\gabriele\\appdata\\roaming\\python\\python312\\site-packages (from ipywidgets->pytorch-tabular) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\gabriele\\appdata\\roaming\\python\\python312\\site-packages (from ipywidgets->pytorch-tabular) (8.24.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\gabriele\\appdata\\roaming\\python\\python312\\site-packages (from ipywidgets->pytorch-tabular) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.10 in c:\\users\\gabriele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipywidgets->pytorch-tabular) (4.0.10)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.10 in c:\\users\\gabriele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipywidgets->pytorch-tabular) (3.0.10)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\gabriele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\gabriele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\gabriele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning<2.2.0,>=2.0.0->pytorch-tabular) (3.9.5)\n",
      "Requirement already satisfied: decorator in c:\\users\\gabriele\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets->pytorch-tabular) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\gabriele\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets->pytorch-tabular) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\gabriele\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets->pytorch-tabular) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\gabriele\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets->pytorch-tabular) (3.0.43)\n",
      "Requirement already satisfied: stack-data in c:\\users\\gabriele\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets->pytorch-tabular) (0.6.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\gabriele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=11.0.0->pytorch-tabular) (0.1.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\gabriele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<2.2.0,>=2.0.0->pytorch-tabular) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\gabriele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<2.2.0,>=2.0.0->pytorch-tabular) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\gabriele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<2.2.0,>=2.0.0->pytorch-tabular) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\gabriele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<2.2.0,>=2.0.0->pytorch-tabular) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\gabriele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<2.2.0,>=2.0.0->pytorch-tabular) (1.9.4)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\gabriele\\appdata\\roaming\\python\\python312\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets->pytorch-tabular) (0.8.4)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\gabriele\\appdata\\roaming\\python\\python312\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets->pytorch-tabular) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\gabriele\\appdata\\roaming\\python\\python312\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets->pytorch-tabular) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\gabriele\\appdata\\roaming\\python\\python312\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets->pytorch-tabular) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\gabriele\\appdata\\roaming\\python\\python312\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets->pytorch-tabular) (0.2.2)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\gabriele\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<2.2.0,>=2.0.0->pytorch-tabular) (3.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy pandas scikit-learn torch pytorch-tabnet pytorch-tabular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import e definizioni delle funzioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# import torch\n",
    "# from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "# from pytorch_tabular.models import TabTransformerModel\n",
    "# from pytorch_tabular.config import ModelConfig, DataConfig, TrainerConfig\n",
    "# from pytorch_tabular import TabularModel\n",
    "import pickle\n",
    "\n",
    "# Funzioni di utilit√†\n",
    "def save_model(model, directory, filename):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    with open(os.path.join(directory, filename + '.pkl'), 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "def cross_val_score_with_preprocessing(model, X, y, cv, scaler_type, use_pca, n_components):\n",
    "    mse_scores, mae_scores, r2_scores = [], [], []\n",
    "    for train_idx, val_idx in KFold(n_splits=cv, shuffle=True, random_state=89).split(X):\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        # Scaling\n",
    "        if scaler_type == 'Standard':\n",
    "            scaler = StandardScaler().fit(X_train)\n",
    "        elif scaler_type == 'MinMax':\n",
    "            scaler = MinMaxScaler().fit(X_train)\n",
    "        X_train_scaled = scaler.transform(X_train)\n",
    "        X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "        # PCA\n",
    "        if use_pca:\n",
    "            pca = PCA(n_components=n_components).fit(X_train_scaled)\n",
    "            X_train_scaled = pca.transform(X_train_scaled)\n",
    "            X_val_scaled = pca.transform(X_val_scaled)\n",
    "        \n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_val_scaled)\n",
    "        mse_scores.append(mean_squared_error(y_val, y_pred))\n",
    "        mae_scores.append(mean_absolute_error(y_val, y_pred))\n",
    "        r2_scores.append(r2_score(y_val, y_pred))\n",
    "        \n",
    "    return np.mean(mse_scores), np.mean(mae_scores), np.mean(r2_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_state = 89\n",
    "\n",
    "# def save_model(model, directory, filename):\n",
    "#     if not os.path.exists(directory):\n",
    "#         os.makedirs(directory)\n",
    "#     joblib.dump(model, os.path.join(directory, f'{filename}.pkl'))\n",
    "\n",
    "# def preprocess(X, scaler_type='standard', use_pca=False, n_components=None):\n",
    "#     if scaler_type == 'standard':\n",
    "#         scaler = StandardScaler()\n",
    "#     elif scaler_type == 'minmax':\n",
    "#         scaler = MinMaxScaler()\n",
    "#     X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "#     pca = None\n",
    "#     if use_pca and n_components:\n",
    "#         pca = PCA(n_components=n_components, random_state=random_state)\n",
    "#         X_scaled = pca.fit_transform(X_scaled)\n",
    "\n",
    "#     return X_scaled, scaler, pca\n",
    "\n",
    "# def cross_val_score_with_preprocessing(model, X, y, cv=5, scaler_type='standard', use_pca=False, n_components=None):\n",
    "#     kf = KFold(n_splits=cv, shuffle=True, random_state=random_state)\n",
    "#     mse_scores = []\n",
    "#     mae_scores = []\n",
    "#     r2_scores = []\n",
    "\n",
    "#     for train_index, test_index in kf.split(X):\n",
    "#         X_train, X_test = X[train_index], X[test_index]\n",
    "#         y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "#         X_train_scaled, scaler, pca = preprocess(X_train, scaler_type=scaler_type, use_pca=use_pca, n_components=n_components)\n",
    "#         X_test_scaled = scaler.transform(X_test)\n",
    "#         if pca:\n",
    "#             X_test_scaled = pca.transform(X_test_scaled)\n",
    "\n",
    "#         model.fit(X_train_scaled, y_train)\n",
    "#         y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "#         mse_scores.append(mean_squared_error(y_test, y_pred))\n",
    "#         mae_scores.append(mean_absolute_error(y_test, y_pred))\n",
    "#         r2_scores.append(r2_score(y_test, y_pred))\n",
    "\n",
    "#     return np.mean(mse_scores), np.mean(mae_scores), np.mean(r2_scores)\n",
    "\n",
    "\n",
    "\n",
    "# def grid_search_cv_with_preprocessing(model, param_grid, X, y, cv=5, scaler_type='standard', use_pca=False, n_components=None):\n",
    "#     # Creazione del pipeline di preprocessing\n",
    "#     steps = []\n",
    "#     if scaler_type == 'standard':\n",
    "#         steps.append(('scaler', StandardScaler()))\n",
    "#     elif scaler_type == 'minmax':\n",
    "#         steps.append(('scaler', MinMaxScaler()))\n",
    "#     if use_pca:\n",
    "#         steps.append(('pca', PCA(n_components=n_components)))\n",
    "\n",
    "#     pipeline = Pipeline(steps + [('model', model)])\n",
    "    \n",
    "#     # Grid Search CV\n",
    "#     grid_search = GridSearchCV(pipeline, param_grid, cv=cv, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "#     grid_search.fit(X, y)\n",
    "    \n",
    "#     best_model = grid_search.best_estimator_\n",
    "#     best_params = grid_search.best_params_\n",
    "#     best_score = -grid_search.best_score_\n",
    "    \n",
    "#     return best_model, best_params, best_score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def save_performance(model_name, scaler_type, use_pca, mse, mae, r2, mse_cv, mae_cv, r2_cv, filename='model_performance.csv'):\n",
    "#     file_exists = os.path.isfile(filename)\n",
    "#     with open(filename, mode='a', newline='') as file:\n",
    "#         writer = csv.writer(file)\n",
    "#         if not file_exists:\n",
    "#             writer.writerow(['Model', 'Scaler', 'PCA', 'MSE', 'MAE', 'R2', 'CV_MSE', 'CV_MAE', 'CV_R2'])\n",
    "#         writer.writerow([model_name, scaler_type, use_pca, mse, mae, r2, mse_cv, mae_cv, r2_cv])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caricamento dei Dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurazione dei modelli\n",
    "preprocessing_options = {\n",
    "    'LR': {'scaler_type': ['Standard', 'MinMax'], 'use_pca': [True, False]},\n",
    "    'RF': {'scaler_type': ['Standard', 'MinMax'], 'use_pca': [True, False]},\n",
    "    'KNR': {'scaler_type': ['Standard', 'MinMax'], 'use_pca': [True, False]},\n",
    "    'SVR': {'scaler_type': ['Standard', 'MinMax'], 'use_pca': [True, False]},\n",
    "    'FFNN': {'scaler_type': ['Standard', 'MinMax'], 'use_pca': [True, False]},\n",
    "    'TabNet': {'scaler_type': ['Standard', 'MinMax'], 'use_pca': [False]},\n",
    "    'TabTransformer': {'scaler_type': ['Standard', 'MinMax'], 'use_pca': [False]}\n",
    "}\n",
    "\n",
    "# CSV zip folder's path\n",
    "csv_file_name = '../data.zip'\n",
    "# loading data from csv\n",
    "df = pd.read_csv(csv_file_name)\n",
    "\n",
    "X = df.drop('Year', axis=1)\n",
    "y = df['Year']\n",
    "\n",
    "# Divisione in training e validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=89)\n",
    "\n",
    "# Creazione degli scaler e PCA per ogni combinazione di preprocessing\n",
    "scalers = {}\n",
    "pcas = {}\n",
    "\n",
    "for model in preprocessing_options:\n",
    "    for scaler_type in preprocessing_options[model]['scaler_type']:\n",
    "        for use_pca in preprocessing_options[model]['use_pca']:\n",
    "            key = f\"{model}_{scaler_type}_{'PCA' if use_pca else 'NoPCA'}\"\n",
    "            if scaler_type == 'Standard':\n",
    "                scaler = StandardScaler().fit(X_train)\n",
    "            elif scaler_type == 'MinMax':\n",
    "                scaler = MinMaxScaler().fit(X_train)\n",
    "            scalers[key] = scaler\n",
    "            if use_pca:\n",
    "                pca = PCA(n_components=52).fit(scaler.transform(X_train))\n",
    "                pcas[key] = pca\n",
    "            else:\n",
    "                pcas[key] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funzioni di Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per addestrare i modelli\n",
    "def train_model(X_train_scaled, y_train, model_type):\n",
    "    if model_type == 'LR':\n",
    "        model = LinearRegression()\n",
    "    elif model_type == 'RF':\n",
    "        model = RandomForestRegressor(random_state=89)\n",
    "    elif model_type == 'KNR':\n",
    "        model = KNeighborsRegressor()\n",
    "    elif model_type == 'SVR':\n",
    "        model = SVR()\n",
    "    return model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Funzione per addestrare una rete neurale feed-forward\n",
    "def train_ffnn(X_train_scaled, y_train, input_dim):\n",
    "    class FFNN(torch.nn.Module):\n",
    "        def __init__(self, input_dim):\n",
    "            super(FFNN, self).__init__()\n",
    "            self.fc1 = torch.nn.Linear(input_dim, 128)\n",
    "            self.fc2 = torch.nn.Linear(128, 64)\n",
    "            self.fc3 = torch.nn.Linear(64, 1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = torch.relu(self.fc1(x))\n",
    "            x = torch.relu(self.fc2(x))\n",
    "            x = self.fc3(x)\n",
    "            return x\n",
    "\n",
    "    ffnn = FFNN(input_dim=input_dim)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(ffnn.parameters(), lr=0.001)\n",
    "\n",
    "    X_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    ffnn.train()\n",
    "    for epoch in range(100):  # Adjust the number of epochs as needed\n",
    "        optimizer.zero_grad()\n",
    "        outputs = ffnn(X_tensor)\n",
    "        loss = criterion(outputs, y_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return ffnn\n",
    "\n",
    "# Funzione per addestrare TabNet\n",
    "def train_tabnet(X_train_scaled, y_train, X_val_scaled, y_val):\n",
    "    tabnet = TabNetRegressor()\n",
    "    tabnet.fit(X_train_scaled, y_train, eval_set=[(X_val_scaled, y_val)], patience=10, max_epochs=100)\n",
    "    return tabnet\n",
    "\n",
    "# Funzione per addestrare TabTransformer\n",
    "def train_tabtransformer(df_train, df_val):\n",
    "    data_config = DataConfig(\n",
    "        target=['target_column'],\n",
    "        continuous_cols=df_train.columns.difference(['target_column']).tolist(),\n",
    "    )\n",
    "\n",
    "    model_config = ModelConfig(\n",
    "        task=\"regression\",\n",
    "        metrics=[\"mean_squared_error\", \"mean_absolute_error\", \"r2_score\"],\n",
    "        metrics_params=[{}, {}, {}]\n",
    "    )\n",
    "\n",
    "    trainer_config = TrainerConfig(\n",
    "        max_epochs=100,\n",
    "        gpus=0\n",
    "    )\n",
    "\n",
    "    tabular_model = TabularModel(\n",
    "        data_config=data_config,\n",
    "        model_config=model_config,\n",
    "        trainer_config=trainer_config\n",
    "    )\n",
    "\n",
    "    tabular_model.fit(train=df_train, validation=df_val)\n",
    "    return tabular_model\n",
    "\n",
    "# Preparazione del dizionario per salvare le performance\n",
    "performance_dict = {\n",
    "    'Model': [],\n",
    "    'Scaler': [],\n",
    "    'PCA': [],\n",
    "    'MSE_Val': [],\n",
    "    'MAE_Val': [],\n",
    "    'R2_Val': [],\n",
    "    'MSE_CV': [],\n",
    "    'MAE_CV': [],\n",
    "    'R2_CV': []\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing e salvataggio dei risultati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esegui il preprocessing specifico per ogni modello con Standard Scaler e Min-Max Scaler\n",
    "preprocessing_options = {\n",
    "    'LR': {'scaler_type': ['standard', 'minmax'], 'use_pca': [True, False], 'n_components': 52},\n",
    "    'RF': {'scaler_type': ['standard', 'minmax'], 'use_pca': [True, False], 'n_components': 52},\n",
    "    'KNR': {'scaler_type': ['standard', 'minmax'], 'use_pca': [True, False], 'n_components': 52},\n",
    "    'SVR': {'scaler_type': ['standard', 'minmax'], 'use_pca': [True, False], 'n_components': 52},\n",
    "    'FFNN': {'scaler_type': ['standard', 'minmax'], 'use_pca': [True, False], 'n_components': 52},\n",
    "    'TabNet': {'scaler_type': ['standard', 'minmax'], 'use_pca': [False], 'n_components': None},\n",
    "    'TabTransformer': {'scaler_type': ['standard', 'minmax'], 'use_pca': [False], 'n_components': None},\n",
    "}\n",
    "\n",
    "# Funzione per eseguire il preprocessing e salvare i risultati\n",
    "def preprocess_and_save(X_train, X_val, preprocessing_options):\n",
    "    scalers = {}\n",
    "    pcas = {}\n",
    "    for clfName, options in preprocessing_options.items():\n",
    "        for scaler_type in options['scaler_type']:\n",
    "            for use_pca in options['use_pca']:\n",
    "                key = f\"{clfName}_{scaler_type}_{'PCA' if use_pca else 'NoPCA'}\"\n",
    "                X_train_scaled, scaler, pca = preprocess(X_train, scaler_type=scaler_type, use_pca=use_pca, n_components=options['n_components'])\n",
    "                X_val_scaled = scaler.transform(X_val)\n",
    "                if pca:\n",
    "                    X_val_scaled = pca.transform(X_val_scaled)\n",
    "                scalers[key] = scaler\n",
    "                pcas[key] = pca\n",
    "                directory = os.path.join('models', key.lower())\n",
    "                save_model(scaler, directory, 'scaler')\n",
    "                if pca:\n",
    "                    save_model(pca, directory, 'pca')\n",
    "    return scalers, pcas\n",
    "\n",
    "# Esegui il preprocessing per tutte le combinazioni di scaler e PCA\n",
    "scalers, pcas = preprocess_and_save(X_train, X_val, preprocessing_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training trad models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All the models (DO NOT run this cell if you don't want your PC to crash or to explode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addestra i modelli tradizionali con tutte le combinazioni di scaler e PCA\n",
    "models = {}\n",
    "cv_performance = {}\n",
    "validation_performance = {}\n",
    "\n",
    "for clfName in ['LR', 'RF', 'KNR', 'SVR']:\n",
    "    for scaler_type in preprocessing_options[clfName]['scaler_type']:\n",
    "        for use_pca in preprocessing_options[clfName]['use_pca']:\n",
    "            key = f\"{clfName}_{scaler_type}_{'PCA' if use_pca else 'NoPCA'}\"\n",
    "            X_train_scaled = scalers[key].transform(X_train)\n",
    "            if pcas[key]:\n",
    "                X_train_scaled = pcas[key].transform(X_train_scaled)\n",
    "            \n",
    "            model = train_model(X_train_scaled, y_train, model_type=clfName)\n",
    "            models[key] = model\n",
    "            save_model(model, f'model_{key.lower()}')\n",
    "\n",
    "            # Valutazione su validation set\n",
    "            X_val_scaled = scalers[key].transform(X_val)\n",
    "            if pcas[key]:\n",
    "                X_val_scaled = pcas[key].transform(X_val_scaled)\n",
    "            y_pred = model.predict(X_val_scaled)\n",
    "            mse = mean_squared_error(y_val, y_pred)\n",
    "            mae = mean_absolute_error(y_val, y_pred)\n",
    "            r2 = r2_score(y_val, y_pred)\n",
    "            validation_performance[key] = {'mse': mse, 'mae': mae, 'r2': r2}\n",
    "            print(f\"Validation Performance of {clfName} with {scaler_type} scaler and PCA={use_pca}: MSE={mse}, MAE={mae}, R2={r2}\")\n",
    "\n",
    "            # Cross-validation\n",
    "            mse_cv, mae_cv, r2_cv = cross_val_score_with_preprocessing(\n",
    "                model, X.values, y, cv=5, scaler_type=scaler_type, use_pca=use_pca, n_components=preprocessing_options[clfName]['n_components']\n",
    "            )\n",
    "            cv_performance[key] = {'mse': mse_cv, 'mae': mae_cv, 'r2': r2_cv}\n",
    "            print(f\"Cross-Validation Performance of {clfName} with {scaler_type} scaler and PCA={use_pca}: MSE={mse_cv}, MAE={mae_cv}, R2={r2_cv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "performance = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Performance of LR with Standard scaler and PCA: MSE=98.2318351539259, MAE=7.392438758148582, R2=0.12050186127982243\n",
      "Validation Performance of LR with Standard scaler and NoPCA: MSE=85.58234834475938, MAE=6.668881118801984, R2=0.23375638907108864\n",
      "Validation Performance of LR with MinMax scaler and PCA: MSE=87.21013510994777, MAE=6.73281651718109, R2=0.21918234158461758\n",
      "Validation Performance of LR with MinMax scaler and NoPCA: MSE=85.58234834475938, MAE=6.668881118801991, R2=0.23375638907108864\n"
     ]
    }
   ],
   "source": [
    "clfName = 'LR'\n",
    "for scaler_type in preprocessing_options[clfName]['scaler_type']:\n",
    "    for use_pca in preprocessing_options[clfName]['use_pca']:\n",
    "        key = f\"{clfName}_{scaler_type}_{'PCA' if use_pca else 'NoPCA'}\"\n",
    "        X_train_scaled = scalers[key].transform(X_train)\n",
    "        if pcas[key]:\n",
    "            X_train_scaled = pcas[key].transform(X_train_scaled)\n",
    "\n",
    "        model = train_model(X_train_scaled, y_train, model_type=clfName)\n",
    "        directory = os.path.join('models', key.lower())\n",
    "        save_model(model, directory, 'model')\n",
    "\n",
    "        # Valutazione su validation set\n",
    "        X_val_scaled = scalers[key].transform(X_val)\n",
    "        if pcas[key]:\n",
    "            X_val_scaled = pcas[key].transform(X_val_scaled)\n",
    "        y_pred = model.predict(X_val_scaled)\n",
    "        mse = mean_squared_error(y_val, y_pred)\n",
    "        mae = mean_absolute_error(y_val, y_pred)\n",
    "        r2 = r2_score(y_val, y_pred)\n",
    "        print(f\"Validation Performance of {clfName} with {scaler_type} scaler and {'PCA' if use_pca else 'NoPCA'}: MSE={mse}, MAE={mae}, R2={r2}\")\n",
    "\n",
    "        # Salvare le performance\n",
    "        performance_dict['Model'].append(clfName)\n",
    "        performance_dict['Scaler'].append(scaler_type)\n",
    "        performance_dict['PCA'].append(use_pca)\n",
    "        performance_dict['MSE_Val'].append(mse)\n",
    "        performance_dict['MAE_Val'].append(mae)\n",
    "        performance_dict['R2_Val'].append(r2)\n",
    "        performance_dict['MSE_CV'].append(None)  # No cross-validation for Linear Regression\n",
    "        performance_dict['MAE_CV'].append(None)\n",
    "        performance_dict['R2_CV'].append(None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Performance of RF with Standard scaler and PCA: MSE=92.01214625373923, MAE=7.210540541101935, R2=0.17618854169825393\n",
      "Cross-Validation Performance of RF with Standard scaler and PCA: MSE=90.85271300574892, MAE=7.169012839722981, R2=0.17553096742065438\n",
      "Validation Performance of RF with Standard scaler and NoPCA: MSE=79.40827703444918, MAE=6.436896507055287, R2=0.2890346419636943\n",
      "Cross-Validation Performance of RF with Standard scaler and NoPCA: MSE=78.4878556946283, MAE=6.400234070255443, R2=0.28774069750333975\n",
      "Validation Performance of RF with MinMax scaler and PCA: MSE=81.65413718735886, MAE=6.573940451406099, R2=0.26892680399838764\n",
      "Cross-Validation Performance of RF with MinMax scaler and PCA: MSE=80.79298888304402, MAE=6.526083350469959, R2=0.2668224632088366\n",
      "Validation Performance of RF with MinMax scaler and NoPCA: MSE=79.42058209449787, MAE=6.438306572816495, R2=0.28892447118868336\n",
      "Cross-Validation Performance of RF with MinMax scaler and NoPCA: MSE=78.52371315641918, MAE=6.401475427778328, R2=0.2874174746901928\n"
     ]
    }
   ],
   "source": [
    "clfName = 'RF'\n",
    "for scaler_type in preprocessing_options[clfName]['scaler_type']:\n",
    "    for use_pca in preprocessing_options[clfName]['use_pca']:\n",
    "        key = f\"{clfName}_{scaler_type}_{'PCA' if use_pca else 'NoPCA'}\"\n",
    "        X_train_scaled = scalers[key].transform(X_train)\n",
    "        if pcas[key]:\n",
    "            X_train_scaled = pcas[key].transform(X_train_scaled)\n",
    "\n",
    "        model = train_model(X_train_scaled, y_train, model_type=clfName)\n",
    "        directory = os.path.join('models', key.lower())\n",
    "        save_model(model, directory, 'model')\n",
    "\n",
    "        # Valutazione su validation set\n",
    "        X_val_scaled = scalers[key].transform(X_val)\n",
    "        if pcas[key]:\n",
    "            X_val_scaled = pcas[key].transform(X_val_scaled)\n",
    "        y_pred = model.predict(X_val_scaled)\n",
    "        mse = mean_squared_error(y_val, y_pred)\n",
    "        mae = mean_absolute_error(y_val, y_pred)\n",
    "        r2 = r2_score(y_val, y_pred)\n",
    "        print(f\"Validation Performance of {clfName} with {scaler_type} scaler and {'PCA' if use_pca else 'NoPCA'}: MSE={mse}, MAE={mae}, R2={r2}\")\n",
    "\n",
    "        # Cross-validation\n",
    "        mse_cv, mae_cv, r2_cv = cross_val_score_with_preprocessing(\n",
    "            model, X.values, y.values, cv=5, scaler_type=scaler_type, use_pca=use_pca, n_components=52\n",
    "        )\n",
    "        print(f\"Cross-Validation Performance of {clfName} with {scaler_type} scaler and {'PCA' if use_pca else 'NoPCA'}: MSE={mse_cv}, MAE={mae_cv}, R2={r2_cv}\")\n",
    "\n",
    "        # Salvare le performance\n",
    "        performance_dict['Model'].append(clfName)\n",
    "        performance_dict['Scaler'].append(scaler_type)\n",
    "        performance_dict['PCA'].append(use_pca)\n",
    "        performance_dict['MSE_Val'].append(mse)\n",
    "        performance_dict['MAE_Val'].append(mae)\n",
    "        performance_dict['R2_Val'].append(r2)\n",
    "        performance_dict['MSE_CV'].append(mse_cv)\n",
    "        performance_dict['MAE_CV'].append(mae_cv)\n",
    "        performance_dict['R2_CV'].append(r2_cv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training KNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gabriele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "found 0 physical cores < 1\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"c:\\Users\\Gabriele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 282, in _count_physical_cores\n",
      "    raise ValueError(f\"found {cpu_count_physical} physical cores < 1\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Performance of KNR with Standard scaler and PCA: MSE=95.77045226529194, MAE=7.149356597600872, R2=0.14253933686845544\n",
      "Cross-Validation Performance of KNR with Standard scaler and PCA: MSE=94.88380767324279, MAE=7.103034797263805, R2=0.1388833328796148\n",
      "Validation Performance of KNR with Standard scaler and NoPCA: MSE=89.67204282740155, MAE=6.853474769505305, R2=0.19714016705119275\n",
      "Cross-Validation Performance of KNR with Standard scaler and NoPCA: MSE=88.92059514226231, MAE=6.822715177951821, R2=0.19298139456120558\n",
      "Validation Performance of KNR with MinMax scaler and PCA: MSE=85.17326063249726, MAE=6.556490532368397, R2=0.23741907012498586\n",
      "Cross-Validation Performance of KNR with MinMax scaler and PCA: MSE=84.32230474868643, MAE=6.529097650441163, R2=0.23475225724794607\n",
      "Validation Performance of KNR with MinMax scaler and NoPCA: MSE=84.66267790225041, MAE=6.556375532864084, R2=0.24199046554085202\n",
      "Cross-Validation Performance of KNR with MinMax scaler and NoPCA: MSE=83.89033330028748, MAE=6.532507980569051, R2=0.2386649377271592\n"
     ]
    }
   ],
   "source": [
    "clfName = 'KNR'\n",
    "for scaler_type in preprocessing_options[clfName]['scaler_type']:\n",
    "    for use_pca in preprocessing_options[clfName]['use_pca']:\n",
    "        key = f\"{clfName}_{scaler_type}_{'PCA' if use_pca else 'NoPCA'}\"\n",
    "        X_train_scaled = scalers[key].transform(X_train)\n",
    "        if pcas[key]:\n",
    "            X_train_scaled = pcas[key].transform(X_train_scaled)\n",
    "\n",
    "        model = train_model(X_train_scaled, y_train, model_type=clfName)\n",
    "        directory = os.path.join('models', key.lower())\n",
    "        save_model(model, directory, 'model')\n",
    "\n",
    "        # Valutazione su validation set\n",
    "        X_val_scaled = scalers[key].transform(X_val)\n",
    "        if pcas[key]:\n",
    "            X_val_scaled = pcas[key].transform(X_val_scaled)\n",
    "        y_pred = model.predict(X_val_scaled)\n",
    "        mse = mean_squared_error(y_val, y_pred)\n",
    "        mae = mean_absolute_error(y_val, y_pred)\n",
    "        r2 = r2_score(y_val, y_pred)\n",
    "        print(f\"Validation Performance of {clfName} with {scaler_type} scaler and {'PCA' if use_pca else 'NoPCA'}: MSE={mse}, MAE={mae}, R2={r2}\")\n",
    "\n",
    "        # Cross-validation\n",
    "        mse_cv, mae_cv, r2_cv = cross_val_score_with_preprocessing(\n",
    "            model, X.values, y.values, cv=5, scaler_type=scaler_type, use_pca=use_pca, n_components=52\n",
    "        )\n",
    "        print(f\"Cross-Validation Performance of {clfName} with {scaler_type} scaler and {'PCA' if use_pca else 'NoPCA'}: MSE={mse_cv}, MAE={mae_cv}, R2={r2_cv}\")\n",
    "\n",
    "        # Salvare le performance\n",
    "        performance_dict['Model'].append(clfName)\n",
    "        performance_dict['Scaler'].append(scaler_type)\n",
    "        performance_dict['PCA'].append(use_pca)\n",
    "        performance_dict['MSE_Val'].append(mse)\n",
    "        performance_dict['MAE_Val'].append(mae)\n",
    "        performance_dict['R2_Val'].append(r2)\n",
    "        performance_dict['MSE_CV'].append(mse_cv)\n",
    "        performance_dict['MAE_CV'].append(mae_cv)\n",
    "        performance_dict['R2_CV'].append(r2_cv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfName = 'SVR'\n",
    "for scaler_type in preprocessing_options[clfName]['scaler_type']:\n",
    "    for use_pca in preprocessing_options[clfName]['use_pca']:\n",
    "        key = f\"{clfName}_{scaler_type}_{'PCA' if use_pca else 'NoPCA'}\"\n",
    "        X_train_scaled = scalers[key].transform(X_train)\n",
    "        if pcas[key]:\n",
    "            X_train_scaled = pcas[key].transform(X_train_scaled)\n",
    "\n",
    "        model = train_model(X_train_scaled, y_train, model_type=clfName)\n",
    "        directory = os.path.join('models', key.lower())\n",
    "        save_model(model, directory, 'model')\n",
    "\n",
    "        # Valutazione su validation set\n",
    "        X_val_scaled = scalers[key].transform(X_val)\n",
    "        if pcas[key]:\n",
    "            X_val_scaled = pcas[key].transform(X_val_scaled)\n",
    "        y_pred = model.predict(X_val_scaled)\n",
    "        mse = mean_squared_error(y_val, y_pred)\n",
    "        mae = mean_absolute_error(y_val, y_pred)\n",
    "        r2 = r2_score(y_val, y_pred)\n",
    "        print(f\"Validation Performance of {clfName} with {scaler_type} scaler and {'PCA' if use_pca else 'NoPCA'}: MSE={mse}, MAE={mae}, R2={r2}\")\n",
    "\n",
    "        # Cross-validation\n",
    "        mse_cv, mae_cv, r2_cv = cross_val_score_with_preprocessing(\n",
    "            model, X.values, y.values, cv=5, scaler_type=scaler_type, use_pca=use_pca, n_components=52\n",
    "        )\n",
    "        print(f\"Cross-Validation Performance of {clfName} with {scaler_type} scaler and {'PCA' if use_pca else 'NoPCA'}: MSE={mse_cv}, MAE={mae_cv}, R2={r2_cv}\")\n",
    "\n",
    "        # Salvare le performance\n",
    "        performance_dict['Model'].append(clfName)\n",
    "        performance_dict['Scaler'].append(scaler_type)\n",
    "        performance_dict['PCA'].append(use_pca)\n",
    "        performance_dict['MSE_Val'].append(mse)\n",
    "        performance_dict['MAE_Val'].append(mae)\n",
    "        performance_dict['R2_Val'].append(r2)\n",
    "        performance_dict['MSE_CV'].append(mse_cv)\n",
    "        performance_dict['MAE_CV'].append(mae_cv)\n",
    "        performance_dict['R2_CV'].append(r2_cv)\n",
    "\n",
    "\n",
    "# Validation Performance of SVR with Standard scaler and PCA: MSE=89.76062511715364, MAE=6.356003295928954, R2=0.19634706409390346\n",
    "# Cross-Validation Performance of SVR with Standard scaler and PCA: MSE=88.47693285428485, MAE=6.314976936016734, R2=0.19714760071223011\n",
    "# Validation Performance of SVR with Standard scaler and NoPCA: MSE=78.95616094014734, MAE=5.841599492634313, R2=0.2930825686139583\n",
    "# Cross-Validation Performance of SVR with Standard scaler and NoPCA: MSE=77.90689735248935, MAE=5.800933341091231, R2=0.29307553783482143\n",
    "# Validation Performance of SVR with MinMax scaler and PCA: MSE=78.74912613182043, MAE=5.808798686392694, R2=0.29493621135908754\n",
    "# Cross-Validation Performance of SVR with MinMax scaler and PCA: MSE=77.6737074521388, MAE=5.765969088166743, R2=0.29518672879002494\n",
    "# Validation Performance of SVR with MinMax scaler and NoPCA: MSE=87.8756541494357, MAE=6.264030260698204, R2=0.21322375641113178\n",
    "# Cross-Validation Performance of SVR with MinMax scaler and NoPCA: MSE=86.66237617661876, MAE=6.2207294324537274, R2=0.21361395079336915"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV zip folder's path\n",
    "csv_file_name = '../data.zip'\n",
    "# loading data from csv\n",
    "df = pd.read_csv(csv_file_name)\n",
    "\n",
    "X = df.drop('Year', axis=1)\n",
    "y = df['Year']\n",
    "\n",
    "# Divisione in training e validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dati di training normalizzati MinMaxScaling:\n",
      "[[0.85141255 0.54124099 0.51320672 ... 0.46749133 0.40665778 0.42897257]\n",
      " [0.61060493 0.50150322 0.42992334 ... 0.45522043 0.45247919 0.50174392]\n",
      " [0.44105064 0.43994505 0.52900578 ... 0.48328292 0.3835727  0.38158482]\n",
      " ...\n",
      " [0.81026452 0.47977891 0.52228356 ... 0.45482219 0.40558655 0.40335066]\n",
      " [0.54172766 0.54405139 0.27972061 ... 0.50070901 0.41803509 0.39640128]\n",
      " [0.75397911 0.57359645 0.4808505  ... 0.46099608 0.41463365 0.43697254]]\n"
     ]
    }
   ],
   "source": [
    "X_Validation = X_val.copy()\n",
    "\n",
    "# Min-Max Scaling\n",
    "# Creazione dell'oggetto MinMaxScaler e adattamento solo al training set\n",
    "#file = open(\"scaler.save\",\"wb\") #salvataggio dello scaler sul disco nel file \"scaler.save\"\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X_train)\n",
    "#apply transform on validation set\n",
    "X_val_scaled = scaler.transform(X_Validation)\n",
    "#pickle.dump(scaler, file)\n",
    "#file.close()\n",
    "\n",
    "#name columns after min-max scaling\n",
    "# num_colonne = X_scaled.shape[1]  # Ottieni il numero di colonne\n",
    "# nome_colonne = ['S' + str(i) for i in range(num_colonne)]\n",
    "# X_scaled_df = pd.DataFrame(X_scaled, columns=nome_colonne)\n",
    "\n",
    "# print(\"Dati di training originali:\")\n",
    "# print(X)\n",
    "print(\"\\nDati di training normalizzati MinMaxScaling:\")\n",
    "print(X_scaled)\n",
    "# print(\"\\nDati di validation originali:\")\n",
    "# print(X_Validation)\n",
    "# print(\"\\nDati di validation normalizzati Min-Max:\")\n",
    "# print(X_val_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param = {'kernel' : ('linear', 'poly', 'rbf', 'sigmoid'),\n",
    "         'C' : [1,5],'degree' : [3,5],'coef0' : [0.01,0.5],'gamma' : ('auto','scale')}\n",
    "\n",
    "modelsvr = SVR()\n",
    "\n",
    "grids = GridSearchCV(modelsvr,param,cv=5,verbose=2, n_jobs=-1) #verbose=2, n_jobs=-1\n",
    "\n",
    "grids.fit(X_scaled,y_train)\n",
    "\n",
    "y_pred=grids.predict(X_val_scaled)\n",
    "\n",
    "mse = mean_squared_error(y_val, y_pred)\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "r2 = r2_score(y_val, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"R2:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Codice da eseguire Gabri Traning SVR (minMax + PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV zip folder's path\n",
    "csv_file_name = '../data.zip'\n",
    "# loading data from csv\n",
    "df = pd.read_csv(csv_file_name)\n",
    "\n",
    "X = df.drop('Year', axis=1)\n",
    "y = df['Year']\n",
    "\n",
    "# Divisione in training e validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dati di training normalizzati MinMaxScaling:\n",
      "[[0.85141255 0.54124099 0.51320672 ... 0.46749133 0.40665778 0.42897257]\n",
      " [0.61060493 0.50150322 0.42992334 ... 0.45522043 0.45247919 0.50174392]\n",
      " [0.44105064 0.43994505 0.52900578 ... 0.48328292 0.3835727  0.38158482]\n",
      " ...\n",
      " [0.81026452 0.47977891 0.52228356 ... 0.45482219 0.40558655 0.40335066]\n",
      " [0.54172766 0.54405139 0.27972061 ... 0.50070901 0.41803509 0.39640128]\n",
      " [0.75397911 0.57359645 0.4808505  ... 0.46099608 0.41463365 0.43697254]]\n"
     ]
    }
   ],
   "source": [
    "X_Validation = X_val.copy()\n",
    "\n",
    "# Min-Max Scaling\n",
    "# Creazione dell'oggetto MinMaxScaler e adattamento solo al training set\n",
    "#file = open(\"scaler.save\",\"wb\") #salvataggio dello scaler sul disco nel file \"scaler.save\"\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X_train)\n",
    "#apply transform on validation set\n",
    "X_val_scaled = scaler.transform(X_Validation)\n",
    "#pickle.dump(scaler, file)\n",
    "#file.close()\n",
    "\n",
    "#name columns after min-max scaling\n",
    "# num_colonne = X_scaled.shape[1]  # Ottieni il numero di colonne\n",
    "# nome_colonne = ['S' + str(i) for i in range(num_colonne)]\n",
    "# X_scaled_df = pd.DataFrame(X_scaled, columns=nome_colonne)\n",
    "\n",
    "# print(\"Dati di training originali:\")\n",
    "# print(X)\n",
    "print(\"\\nDati di training normalizzati MinMaxScaling:\")\n",
    "print(X_scaled)\n",
    "# print(\"\\nDati di validation originali:\")\n",
    "# print(X_Validation)\n",
    "# print(\"\\nDati di validation normalizzati Min-Max:\")\n",
    "# print(X_val_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inizializzare l'oggetto PCA\n",
    "pca = PCA(n_components=52)\n",
    "X_decomposed = pca.fit_transform(X_scaled)\n",
    "#pca.get_feature_names_out(X_scaled_df.columns)\n",
    "\n",
    "# Ottieni i vettori delle componenti principali\n",
    "components = pca.components_\n",
    "\n",
    "#validation data after PCA\n",
    "X_val_decomposed = pca.transform(X_val_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param = {'kernel' : ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "         'C' : [1,5,10],\n",
    "         'degree' : [3,8],\n",
    "         'coef0' : [0.01,10,0.5],\n",
    "         'gamma' : ['auto','scale']}\n",
    "\n",
    "modelsvr = SVR(cache_size=200)\n",
    "\n",
    "grids = GridSearchCV(modelsvr,param,cv=5,verbose=2, n_jobs=-1) #verbose=2, n_jobs=-1\n",
    "\n",
    "grids.fit(X_decomposed,y_train)\n",
    "\n",
    "y_pred=grids.predict(X_val_decomposed)\n",
    "\n",
    "mse = mean_squared_error(y_val, y_pred)\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "r2 = r2_score(y_val, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"R2:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training FFNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (4225870712.py, line 167)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[3], line 167\u001b[1;36m\u001b[0m\n\u001b[1;33m    train_subset = Subset(my_dataset, train_idx)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Funzione per fissare la casualit√†\n",
    "def fix_random(seed: int) -> None:\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Definizione del Dataset personalizzato\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.FloatTensor(y.values).view(-1, 1)\n",
    "        self.num_features = X.shape[1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx,:], self.y[idx]\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3, dropout_prob):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, hidden_size3)\n",
    "        self.fc4 = nn.Linear(hidden_size3, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.relu(self.fc1(x))\n",
    "        h = self.dropout(h)\n",
    "        h = self.relu(self.fc2(h))\n",
    "        h = self.dropout(h)\n",
    "        h = self.relu(self.fc3(h))\n",
    "        h = self.dropout(h)\n",
    "        output = self.fc4(h)\n",
    "        return output\n",
    "\n",
    "# Funzione per valutare le performance sul validation e test set\n",
    "def test_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_test = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, targets in data_loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            output = model(data)\n",
    "            y_pred.append(output.cpu().numpy())\n",
    "            y_test.append(targets.cpu().numpy())\n",
    "\n",
    "    y_test = np.concatenate(y_test).squeeze()\n",
    "    y_pred = np.concatenate(y_pred).squeeze()\n",
    "\n",
    "    return y_test, y_pred\n",
    "\n",
    "# Funzione per il processo di addestramento\n",
    "def train_model(model, criterion, optimizer, num_epochs, train_loader, val_loader, device, writer, log_name=\"model\", patience=10):\n",
    "    n_iter = 0\n",
    "    best_valid_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    early_stop = False\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        if early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for data, targets in train_loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            y_pred = model(data)\n",
    "\n",
    "            # Compute Loss\n",
    "            loss = criterion(y_pred, targets)\n",
    "            writer.add_scalar(\"Loss/train\", loss.item(), n_iter)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            n_iter += 1\n",
    "\n",
    "        # Validation\n",
    "        labels, y_pred = test_model(model, val_loader, device)\n",
    "        loss_val = criterion(torch.tensor(y_pred), torch.tensor(labels))\n",
    "        writer.add_scalar(\"Loss/val\", loss_val.item(), epoch)\n",
    "\n",
    "        # Save best model\n",
    "        if loss_val.item() < best_valid_loss:\n",
    "            best_valid_loss = loss_val.item()\n",
    "            epochs_no_improve = 0\n",
    "            if not os.path.exists('models'):\n",
    "                os.makedirs('models')\n",
    "            torch.save(model.state_dict(), 'models/'+log_name)\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                early_stop = True\n",
    "\n",
    "    return model\n",
    "\n",
    "# Look for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device: {}\".format(device))\n",
    "\n",
    "seed = 89\n",
    "fix_random(seed)\n",
    "\n",
    "# Train hyperparameters\n",
    "num_epochs = 150\n",
    "initial_learning_rate = 0.001\n",
    "batch_sizes = [8, 16, 32, 64, 128]  # Lista dei batch size da sperimentare\n",
    "dropout_prob = 0.5  # Probabilit√† di dropout\n",
    "\n",
    "# CSV zip folder's path\n",
    "csv_file_name = '../data.zip'\n",
    "# loading data from csv\n",
    "data = pd.read_csv(csv_file_name)\n",
    "\n",
    "# Caricamento del dataset\n",
    "X = data.drop('Year', axis=1)\n",
    "y = data['Year']\n",
    "\n",
    "# Separate indices\n",
    "train_idx, val_idx = train_test_split(np.arange(X.shape[0]), test_size=0.2, stratify=y, random_state=seed)\n",
    "\n",
    "# Scale data with MinMaxScaling\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X.iloc[train_idx,:])\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "pca = PCA(n_components=52)\n",
    "X_scaled2 = pca.fit_transform(X_scaled)\n",
    "\n",
    "# # Create the dataset\n",
    "# my_dataset = MyDataset(X_scaled2, y)\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    print(f\"Training with batch size: {batch_size}\")\n",
    "\n",
    "    # Create the dataset\n",
    "    my_dataset = MyDataset(X_scaled2, y)\n",
    "    \n",
    "    # Create subsets and relative dataloader\n",
    "    train_subset = Subset(my_dataset, train_idx)\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    val_subset = Subset(my_dataset, val_idx)\n",
    "    val_loader = DataLoader(val_subset, batch_size=1)\n",
    "\n",
    "    # Define the architecture, loss, and optimizer\n",
    "    hidden_size1 = 32\n",
    "    hidden_size2 = 32\n",
    "    hidden_size3 = 32\n",
    "    model = FeedForward(my_dataset.num_features, hidden_size1, hidden_size2, hidden_size3, dropout_prob)\n",
    "    model.to(device)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=initial_learning_rate)\n",
    "\n",
    "    # Start tensorboard\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    # Test before the training\n",
    "    y_test, y_pred_c = test_model(model, val_loader, device)\n",
    "    mse = torch.mean((torch.tensor(y_test) - torch.tensor(y_pred_c)) ** 2).item()\n",
    "    mae = mean_absolute_error(y_test, y_pred_c)\n",
    "    r2 = r2_score(y_test, y_pred_c)\n",
    "    print(f\"MSE before training: {mse}\")\n",
    "    print(f\"MAE before training: {mae}\")\n",
    "    print(f\"R2 before training: {r2}\")\n",
    "\n",
    "    # Train the model\n",
    "    model = train_model(model, criterion, optimizer, num_epochs, train_loader, val_loader, device, writer)\n",
    "\n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load(\"models/model\"))\n",
    "    model.to(device)\n",
    "\n",
    "    # Test after the training\n",
    "    y_test, y_pred_c = test_model(model, val_loader, device)\n",
    "    mse = torch.mean((torch.tensor(y_test) - torch.tensor(y_pred_c)) ** 2).item()\n",
    "    mae = mean_absolute_error(y_test, y_pred_c)\n",
    "    r2 = r2_score(y_test, y_pred_c)\n",
    "    print(f\"MSE after training with batch size {batch_size}: {mse}\")\n",
    "    print(f\"MAE after training with batch size {batch_size}: {mae}\")\n",
    "    print(f\"R2 after training with batch size {batch_size}: {r2}\")\n",
    "\n",
    "    # Close tensorboard writer after training\n",
    "    writer.flush()\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training TabNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gabriele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Targets should be 2D : (n_samples, n_regression) but y_train.shape=(201740,) given.\nUse reshape(-1, 1) for single regression.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m X_train_scaled \u001b[38;5;241m=\u001b[39m scalers[key]\u001b[38;5;241m.\u001b[39mtransform(X_train)\n\u001b[0;32m      6\u001b[0m X_val_scaled \u001b[38;5;241m=\u001b[39m scalers[key]\u001b[38;5;241m.\u001b[39mtransform(X_val)\n\u001b[1;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_tabnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m directory \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m'\u001b[39m, key\u001b[38;5;241m.\u001b[39mlower())\n\u001b[0;32m     10\u001b[0m save_model(model, directory, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[16], line 48\u001b[0m, in \u001b[0;36mtrain_tabnet\u001b[1;34m(X_train_scaled, y_train, X_val_scaled, y_val)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_tabnet\u001b[39m(X_train_scaled, y_train, X_val_scaled, y_val):\n\u001b[0;32m     47\u001b[0m     tabnet \u001b[38;5;241m=\u001b[39m TabNetRegressor()\n\u001b[1;32m---> 48\u001b[0m     \u001b[43mtabnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tabnet\n",
      "File \u001b[1;32mc:\\Users\\Gabriele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:220\u001b[0m, in \u001b[0;36mTabModel.fit\u001b[1;34m(self, X_train, y_train, eval_set, eval_name, eval_metric, loss_fn, weights, max_epochs, patience, batch_size, virtual_batch_size, num_workers, drop_last, callbacks, pin_memory, from_unsupervised, warm_start, augmentations, compute_importance)\u001b[0m\n\u001b[0;32m    217\u001b[0m check_input(X_train)\n\u001b[0;32m    218\u001b[0m check_warm_start(warm_start, from_unsupervised)\n\u001b[1;32m--> 220\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_fit_params\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;66;03m# Validate and reformat eval set depending on training data\u001b[39;00m\n\u001b[0;32m    228\u001b[0m eval_names, eval_set \u001b[38;5;241m=\u001b[39m validate_eval_set(eval_set, eval_name, X_train, y_train)\n",
      "File \u001b[1;32mc:\\Users\\Gabriele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_tabnet\\tab_model.py:141\u001b[0m, in \u001b[0;36mTabNetRegressor.update_fit_params\u001b[1;34m(self, X_train, y_train, eval_set, weights)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_train\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    138\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTargets should be 2D : (n_samples, n_regression) \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \\\n\u001b[0;32m    139\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut y_train.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_train\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m given.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \\\n\u001b[0;32m    140\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse reshape(-1, 1) for single regression.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 141\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_dim \u001b[38;5;241m=\u001b[39m y_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreds_mapper \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Targets should be 2D : (n_samples, n_regression) but y_train.shape=(201740,) given.\nUse reshape(-1, 1) for single regression."
     ]
    }
   ],
   "source": [
    "clfName = 'TabNet'\n",
    "for scaler_type in preprocessing_options[clfName]['scaler_type']:\n",
    "    for use_pca in preprocessing_options[clfName]['use_pca']:\n",
    "        key = f\"{clfName}_{scaler_type}_{'PCA' if use_pca else 'NoPCA'}\"\n",
    "        X_train_scaled = scalers[key].transform(X_train)\n",
    "        X_val_scaled = scalers[key].transform(X_val)\n",
    "\n",
    "        model = train_tabnet(X_train_scaled, y_train, X_val_scaled, y_val)\n",
    "        directory = os.path.join('models', key.lower())\n",
    "        save_model(model, directory, 'model')\n",
    "\n",
    "        # Valutazione su validation set\n",
    "        y_pred = model.predict(X_val_scaled)\n",
    "        mse = mean_squared_error(y_val, y_pred)\n",
    "        mae = mean_absolute_error(y_val, y_pred)\n",
    "        r2 = r2_score(y_val, y_pred)\n",
    "        print(f\"Validation Performance of {clfName} with {scaler_type} scaler and {'PCA' if use_pca else 'NoPCA'}: MSE={mse}, MAE={mae}, R2={r2}\")\n",
    "\n",
    "        # Cross-validation\n",
    "        mse_cv, mae_cv, r2_cv = cross_val_score_with_preprocessing(\n",
    "            model, X.values, y.values, cv=5, scaler_type=scaler_type, use_pca=use_pca, n_components=52\n",
    "        )\n",
    "        print(f\"Cross-Validation Performance of {clfName} with {scaler_type} scaler and {'PCA' if use_pca else 'NoPCA'}: MSE={mse_cv}, MAE={mae_cv}, R2={r2_cv}\")\n",
    "\n",
    "        # Salvare le performance\n",
    "        performance_dict['Model'].append(clfName)\n",
    "        performance_dict['Scaler'].append(scaler_type)\n",
    "        performance_dict['PCA'].append(use_pca)\n",
    "        performance_dict['MSE_Val'].append(mse)\n",
    "        performance_dict['MAE_Val'].append(mae)\n",
    "        performance_dict['R2_Val'].append(r2)\n",
    "        performance_dict['MSE_CV'].append(mse_cv)\n",
    "        performance_dict['MAE_CV'].append(mae_cv)\n",
    "        performance_dict['R2_CV'].append(r2_cv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training TabTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TrainerConfig.__init__() got an unexpected keyword argument 'gpus'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m df_val_scaled \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(X_val_scaled, columns\u001b[38;5;241m=\u001b[39mX_val\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m     11\u001b[0m df_val_scaled[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_column\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m y_val\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m---> 13\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_tabtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_val_scaled\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m directory \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m'\u001b[39m, key\u001b[38;5;241m.\u001b[39mlower())\n\u001b[0;32m     15\u001b[0m save_model(model, directory, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[16], line 64\u001b[0m, in \u001b[0;36mtrain_tabtransformer\u001b[1;34m(df_train, df_val)\u001b[0m\n\u001b[0;32m     53\u001b[0m data_config \u001b[38;5;241m=\u001b[39m DataConfig(\n\u001b[0;32m     54\u001b[0m     target\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_column\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     55\u001b[0m     continuous_cols\u001b[38;5;241m=\u001b[39mdf_train\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mdifference([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_column\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[0;32m     56\u001b[0m )\n\u001b[0;32m     58\u001b[0m model_config \u001b[38;5;241m=\u001b[39m ModelConfig(\n\u001b[0;32m     59\u001b[0m     task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregression\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     60\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean_squared_error\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean_absolute_error\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr2_score\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     61\u001b[0m     metrics_params\u001b[38;5;241m=\u001b[39m[{}, {}, {}]\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 64\u001b[0m trainer_config \u001b[38;5;241m=\u001b[39m \u001b[43mTrainerConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[0;32m     67\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m tabular_model \u001b[38;5;241m=\u001b[39m TabularModel(\n\u001b[0;32m     70\u001b[0m     data_config\u001b[38;5;241m=\u001b[39mdata_config,\n\u001b[0;32m     71\u001b[0m     model_config\u001b[38;5;241m=\u001b[39mmodel_config,\n\u001b[0;32m     72\u001b[0m     trainer_config\u001b[38;5;241m=\u001b[39mtrainer_config\n\u001b[0;32m     73\u001b[0m )\n\u001b[0;32m     75\u001b[0m tabular_model\u001b[38;5;241m.\u001b[39mfit(train\u001b[38;5;241m=\u001b[39mdf_train, validation\u001b[38;5;241m=\u001b[39mdf_val)\n",
      "\u001b[1;31mTypeError\u001b[0m: TrainerConfig.__init__() got an unexpected keyword argument 'gpus'"
     ]
    }
   ],
   "source": [
    "clfName = 'TabTransformer'\n",
    "for scaler_type in preprocessing_options[clfName]['scaler_type']:\n",
    "    for use_pca in preprocessing_options[clfName]['use_pca']:\n",
    "        key = f\"{clfName}_{scaler_type}_{'PCA' if use_pca else 'NoPCA'}\"\n",
    "        X_train_scaled = scalers[key].transform(X_train)\n",
    "        X_val_scaled = scalers[key].transform(X_val)\n",
    "\n",
    "        df_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "        df_train_scaled['target_column'] = y_train.values\n",
    "        df_val_scaled = pd.DataFrame(X_val_scaled, columns=X_val.columns)\n",
    "        df_val_scaled['target_column'] = y_val.values\n",
    "\n",
    "        model = train_tabtransformer(df_train_scaled, df_val_scaled)\n",
    "        directory = os.path.join('models', key.lower())\n",
    "        save_model(model, directory, 'model')\n",
    "\n",
    "        # Valutazione su validation set\n",
    "        y_pred = model.predict(X_val_scaled)\n",
    "        mse = mean_squared_error(y_val, y_pred)\n",
    "        mae = mean_absolute_error(y_val, y_pred)\n",
    "        r2 = r2_score(y_val, y_pred)\n",
    "        print(f\"Validation Performance of {clfName} with {scaler_type} scaler and {'PCA' if use_pca else 'NoPCA'}: MSE={mse}, MAE={mae}, R2={r2}\")\n",
    "\n",
    "        # Cross-validation\n",
    "        mse_cv, mae_cv, r2_cv = cross_val_score_with_preprocessing(\n",
    "            model, X.values, y.values, cv=5, scaler_type=scaler_type, use_pca=use_pca, n_components=52\n",
    "        )\n",
    "        print(f\"Cross-Validation Performance of {clfName} with {scaler_type} scaler and {'PCA' if use_pca else 'NoPCA'}: MSE={mse_cv}, MAE={mae_cv}, R2={r2_cv}\")\n",
    "\n",
    "        # Salvare le performance\n",
    "        performance_dict['Model'].append(clfName)\n",
    "        performance_dict['Scaler'].append(scaler_type)\n",
    "        performance_dict['PCA'].append(use_pca)\n",
    "        performance_dict['MSE_Val'].append(mse)\n",
    "        performance_dict['MAE_Val'].append(mae)\n",
    "        performance_dict['R2_Val'].append(r2)\n",
    "        performance_dict['MSE_CV'].append(mse_cv)\n",
    "        performance_dict['MAE_CV'].append(mae_cv)\n",
    "        performance_dict['R2_CV'].append(r2_cv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All the models (DO NOT run this cell if you don't want your PC to crash or to explode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valutazione dei modelli con tutte le combinazioni di scaler e PCA\n",
    "# performance = {}\n",
    "\n",
    "# Valutazione modelli tradizionali\n",
    "for clfName in ['LR', 'RF', 'KNR', 'SVR']:\n",
    "    for scaler_type in preprocessing_options[clfName]['scaler_type']:\n",
    "        for use_pca in preprocessing_options[clfName]['use_pca']:\n",
    "            key = f\"{clfName}_{scaler_type}_{'PCA' if use_pca else 'NoPCA'}\"\n",
    "            X_val_scaled = scalers[key].transform(X_val)\n",
    "            if pcas[key]:\n",
    "                X_val_scaled = pcas[key].transform(X_val_scaled)\n",
    "            y_pred = models[key].predict(X_val_scaled)\n",
    "            mse = mean_squared_error(y_val, y_pred)\n",
    "            mae = mean_absolute_error(y_val, y_pred)\n",
    "            r2 = r2_score(y_val, y_pred)\n",
    "            performance[key] = {'mse': mse, 'mae': mae, 'r2': r2}\n",
    "            print(f\"Performance of {clfName} with {scaler_type} scaler and PCA={use_pca}: MSE={mse}, MAE={mae}, R2={r2}\")\n",
    "\n",
    "# Valutazione FFNN\n",
    "for scaler_type in preprocessing_options['FFNN']['scaler_type']:\n",
    "    for use_pca in preprocessing_options['FFNN']['use_pca']:\n",
    "        key = f\"FFNN_{scaler_type}_{'PCA' if use_pca else 'NoPCA'}\"\n",
    "        X_val_scaled = scalers[key].transform(X_val)\n",
    "        if pcas[key]:\n",
    "            X_val_scaled = pcas[key].transform(X_val_scaled)\n",
    "        X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32)\n",
    "        y_pred_tensor = models[key](X_val_tensor).detach().numpy().squeeze()\n",
    "        mse = mean_squared_error(y_val, y_pred_tensor)\n",
    "        mae = mean_absolute_error(y_val, y_pred_tensor)\n",
    "        r2 = r2_score(y_val, y_pred_tensor)\n",
    "        performance[key] = {'mse': mse, 'mae': mae, 'r2': r2}\n",
    "        print(f\"Performance of FFNN with {scaler_type} scaler and PCA={use_pca}: MSE={mse}, MAE={mae}, R2={r2}\")\n",
    "\n",
    "# Valutazione TabNet\n",
    "for scaler_type in preprocessing_options['TabNet']['scaler_type']:\n",
    "    key = f\"TabNet_{scaler_type}_NoPCA\"\n",
    "    X_val_scaled = scalers[key].transform(X_val)\n",
    "    y_pred = models[key].predict(X_val_scaled)\n",
    "    mse = mean_squared_error(y_val, y_pred)\n",
    "    mae = mean_absolute_error(y_val, y_pred)\n",
    "    r2 = r2_score(y_val, y_pred)\n",
    "    performance[key] = {'mse': mse, 'mae': mae, 'r2': r2}\n",
    "    print(f\"Performance of TabNet with {scaler_type} scaler: MSE={mse}, MAE={mae}, R2={r2}\")\n",
    "\n",
    "# Valutazione TabTransformer\n",
    "for scaler_type in preprocessing_options['TabTransformer']['scaler_type']:\n",
    "    key = f\"TabTransformer_{scaler_type}_NoPCA\"\n",
    "    y_pred = models[key].predict(df_val).squeeze()\n",
    "    mse = mean_squared_error(y_val, y_pred)\n",
    "    mae = mean_absolute_error(y_val, y_pred)\n",
    "    r2 = r2_score(y_val, y_pred)\n",
    "    performance[key] = {'mse': mse, 'mae': mae, 'r2': r2}\n",
    "    print(f\"Performance of TabTransformer with {scaler_type} scaler: MSE={mse}, MAE={mae}, R2={r2}\")\n",
    "\n",
    "# Salva le performance dei modelli\n",
    "with open('performance.txt', 'w') as f:\n",
    "    for clfName, metrics in performance.items():\n",
    "        f.write(f\"{clfName}: MSE={metrics['mse']}, MAE={metrics['mae']}, R2={metrics['r2']}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of LR with standard scaler and PCA=True: MSE=98.2318351539259, MAE=7.392438758148582, R2=0.12050186127982243\n",
      "Performance of LR with standard scaler and PCA=False: MSE=85.58234834475938, MAE=6.668881118801984, R2=0.23375638907108864\n",
      "Performance of LR with minmax scaler and PCA=True: MSE=87.21013510994777, MAE=6.73281651718109, R2=0.21918234158461758\n",
      "Performance of LR with minmax scaler and PCA=False: MSE=85.58234834475938, MAE=6.668881118801991, R2=0.23375638907108864\n"
     ]
    }
   ],
   "source": [
    "# Valutazione dei modelli con tutte le combinazioni di scaler e PCA\n",
    "# performance = {}\n",
    "\n",
    "# Valutazione modelli tradizionali\n",
    "for clfName in ['LR']:\n",
    "    for scaler_type in preprocessing_options[clfName]['scaler_type']:\n",
    "        for use_pca in preprocessing_options[clfName]['use_pca']:\n",
    "            key = f\"{clfName}_{scaler_type}_{'PCA' if use_pca else 'NoPCA'}\"\n",
    "            X_val_scaled = scalers[key].transform(X_val)\n",
    "            if pcas[key]:\n",
    "                X_val_scaled = pcas[key].transform(X_val_scaled)\n",
    "            y_pred = models[key].predict(X_val_scaled)\n",
    "            mse = mean_squared_error(y_val, y_pred)\n",
    "            mae = mean_absolute_error(y_val, y_pred)\n",
    "            r2 = r2_score(y_val, y_pred)\n",
    "            performance[key] = {'mse': mse, 'mae': mae, 'r2': r2}\n",
    "            print(f\"Performance of {clfName} with {scaler_type} scaler and PCA={use_pca}: MSE={mse}, MAE={mae}, R2={r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation RF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of RF with standard scaler and PCA=True: MSE=92.01214625373923, MAE=7.210540541101935, R2=0.17618854169825393\n",
      "Performance of RF with standard scaler and PCA=False: MSE=79.40827703444918, MAE=6.436896507055287, R2=0.2890346419636943\n",
      "Performance of RF with minmax scaler and PCA=True: MSE=81.65413718735886, MAE=6.573940451406099, R2=0.26892680399838764\n",
      "Performance of RF with minmax scaler and PCA=False: MSE=79.42058209449787, MAE=6.438306572816495, R2=0.28892447118868336\n"
     ]
    }
   ],
   "source": [
    "# Valutazione dei modelli con tutte le combinazioni di scaler e PCA\n",
    "performance = {}\n",
    "\n",
    "# Valutazione modelli tradizionali\n",
    "for clfName in ['RF']:\n",
    "    for scaler_type in preprocessing_options[clfName]['scaler_type']:\n",
    "        for use_pca in preprocessing_options[clfName]['use_pca']:\n",
    "            key = f\"{clfName}_{scaler_type}_{'PCA' if use_pca else 'NoPCA'}\"\n",
    "            X_val_scaled = scalers[key].transform(X_val)\n",
    "            if pcas[key]:\n",
    "                X_val_scaled = pcas[key].transform(X_val_scaled)\n",
    "            y_pred = models[key].predict(X_val_scaled)\n",
    "            mse = mean_squared_error(y_val, y_pred)\n",
    "            mae = mean_absolute_error(y_val, y_pred)\n",
    "            r2 = r2_score(y_val, y_pred)\n",
    "            performance[key] = {'mse': mse, 'mae': mae, 'r2': r2}\n",
    "            print(f\"Performance of {clfName} with {scaler_type} scaler and PCA={use_pca}: MSE={mse}, MAE={mae}, R2={r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation KNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of KNR with standard scaler and PCA=True: MSE=95.77045226529194, MAE=7.149356597600872, R2=0.14253933686845544\n",
      "Performance of KNR with standard scaler and PCA=False: MSE=89.67204282740155, MAE=6.853474769505305, R2=0.19714016705119275\n",
      "Performance of KNR with minmax scaler and PCA=True: MSE=85.17326063249726, MAE=6.556490532368397, R2=0.23741907012498586\n",
      "Performance of KNR with minmax scaler and PCA=False: MSE=84.66267790225041, MAE=6.556375532864084, R2=0.24199046554085202\n"
     ]
    }
   ],
   "source": [
    "# Valutazione modelli tradizionali\n",
    "for clfName in ['KNR']:\n",
    "    for scaler_type in preprocessing_options[clfName]['scaler_type']:\n",
    "        for use_pca in preprocessing_options[clfName]['use_pca']:\n",
    "            key = f\"{clfName}_{scaler_type}_{'PCA' if use_pca else 'NoPCA'}\"\n",
    "            X_val_scaled = scalers[key].transform(X_val)\n",
    "            if pcas[key]:\n",
    "                X_val_scaled = pcas[key].transform(X_val_scaled)\n",
    "            y_pred = models[key].predict(X_val_scaled)\n",
    "            mse = mean_squared_error(y_val, y_pred)\n",
    "            mae = mean_absolute_error(y_val, y_pred)\n",
    "            r2 = r2_score(y_val, y_pred)\n",
    "            performance[key] = {'mse': mse, 'mae': mae, 'r2': r2}\n",
    "            print(f\"Performance of {clfName} with {scaler_type} scaler and PCA={use_pca}: MSE={mse}, MAE={mae}, R2={r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of SVR with standard scaler and PCA=True: MSE=89.76062511715364, MAE=6.356003295928954, R2=0.19634706409390346\n",
      "Performance of SVR with standard scaler and PCA=False: MSE=78.95616094014734, MAE=5.841599492634313, R2=0.2930825686139583\n",
      "Performance of SVR with minmax scaler and PCA=True: MSE=78.74912613182043, MAE=5.808798686392694, R2=0.29493621135908754\n",
      "Performance of SVR with minmax scaler and PCA=False: MSE=87.8756541494357, MAE=6.264030260698204, R2=0.21322375641113178\n"
     ]
    }
   ],
   "source": [
    "# Valutazione modelli tradizionali\n",
    "for clfName in ['SVR']:\n",
    "    for scaler_type in preprocessing_options[clfName]['scaler_type']:\n",
    "        for use_pca in preprocessing_options[clfName]['use_pca']:\n",
    "            key = f\"{clfName}_{scaler_type}_{'PCA' if use_pca else 'NoPCA'}\"\n",
    "            X_val_scaled = scalers[key].transform(X_val)\n",
    "            if pcas[key]:\n",
    "                X_val_scaled = pcas[key].transform(X_val_scaled)\n",
    "            y_pred = models[key].predict(X_val_scaled)\n",
    "            mse = mean_squared_error(y_val, y_pred)\n",
    "            mae = mean_absolute_error(y_val, y_pred)\n",
    "            r2 = r2_score(y_val, y_pred)\n",
    "            performance[key] = {'mse': mse, 'mae': mae, 'r2': r2}\n",
    "            print(f\"Performance of {clfName} with {scaler_type} scaler and PCA={use_pca}: MSE={mse}, MAE={mae}, R2={r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation FFNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of FFNN with standard scaler and PCA=True: MSE=3827140.8363093333, MAE=1956.1712132502987, R2=-34264.50390625\n",
      "Performance of FFNN with standard scaler and PCA=False: MSE=3606756.0399361844, MAE=1897.8917027504365, R2=-32291.3359375\n",
      "Performance of FFNN with minmax scaler and PCA=True: MSE=3965149.11985667, MAE=1991.2404726114385, R2=-35500.12890625\n",
      "Performance of FFNN with minmax scaler and PCA=False: MSE=3461327.940968171, MAE=1860.4338750381446, R2=-30989.271484375\n"
     ]
    }
   ],
   "source": [
    "# Valutazione FFNN\n",
    "for scaler_type in preprocessing_options['FFNN']['scaler_type']:\n",
    "    for use_pca in preprocessing_options['FFNN']['use_pca']:\n",
    "        key = f\"FFNN_{scaler_type}_{'PCA' if use_pca else 'NoPCA'}\"\n",
    "        X_val_scaled = scalers[key].transform(X_val)\n",
    "        if pcas[key]:\n",
    "            X_val_scaled = pcas[key].transform(X_val_scaled)\n",
    "        X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32)\n",
    "        y_pred_tensor = models[key](X_val_tensor).detach().numpy().squeeze()\n",
    "        mse = mean_squared_error(y_val, y_pred_tensor)\n",
    "        mae = mean_absolute_error(y_val, y_pred_tensor)\n",
    "        r2 = r2_score(y_val, y_pred_tensor)\n",
    "        performance[key] = {'mse': mse, 'mae': mae, 'r2': r2}\n",
    "        print(f\"Performance of FFNN with {scaler_type} scaler and PCA={use_pca}: MSE={mse}, MAE={mae}, R2={r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation TabNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valutazione TabNet\n",
    "for scaler_type in preprocessing_options['TabNet']['scaler_type']:\n",
    "    key = f\"TabNet_{scaler_type}_NoPCA\"\n",
    "    X_val_scaled = scalers[key].transform(X_val)\n",
    "    y_pred = models[key].predict(X_val_scaled)\n",
    "    mse = mean_squared_error(y_val, y_pred)\n",
    "    mae = mean_absolute_error(y_val, y_pred)\n",
    "    r2 = r2_score(y_val, y_pred)\n",
    "    performance[key] = {'mse': mse, 'mae': mae, 'r2': r2}\n",
    "    print(f\"Performance of TabNet with {scaler_type} scaler: MSE={mse}, MAE={mae}, R2={r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation TabTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valutazione TabTransformer\n",
    "for scaler_type in preprocessing_options['TabTransformer']['scaler_type']:\n",
    "    key = f\"TabTransformer_{scaler_type}_NoPCA\"\n",
    "    y_pred = models[key].predict(df_val).squeeze()\n",
    "    mse = mean_squared_error(y_val, y_pred)\n",
    "    mae = mean_absolute_error(y_val, y_pred)\n",
    "    r2 = r2_score(y_val, y_pred)\n",
    "    performance[key] = {'mse': mse, 'mae': mae, 'r2': r2}\n",
    "    print(f\"Performance of TabTransformer with {scaler_type} scaler: MSE={mse}, MAE={mae}, R2={r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvataggio delle performance in un file CSV\n",
    "performance_df = pd.DataFrame(performance_dict)\n",
    "performance_df.to_csv('performance_results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
