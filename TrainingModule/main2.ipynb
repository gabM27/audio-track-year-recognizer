{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installazione delle Librerie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.24.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: pandas in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.0.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.3.0)\n",
      "Requirement already satisfied: torch in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.3.0)\n",
      "Requirement already satisfied: pytorch-tabnet in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (4.1.0)\n",
      "Requirement already satisfied: pytorch-tabular in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (1.11.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (1.3.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch) (2023.12.2)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch) (2021.4.0)\n",
      "Requirement already satisfied: tqdm>=4.36 in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pytorch-tabnet) (4.66.4)\n",
      "Requirement already satisfied: pytorch-lightning<2.2.0,>=2.0.0 in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pytorch-tabular) (2.1.4)\n",
      "Requirement already satisfied: omegaconf>=2.3.0 in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pytorch-tabular) (2.3.0)\n",
      "Requirement already satisfied: torchmetrics<1.3.0,>=0.10.0 in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pytorch-tabular) (1.2.1)\n",
      "Requirement already satisfied: tensorboard!=2.5.0,>2.2.0 in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pytorch-tabular) (2.16.2)\n",
      "Requirement already satisfied: protobuf<4.26.0,>=3.20.0 in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pytorch-tabular) (4.23.4)\n",
      "Requirement already satisfied: PyYAML<6.1.0,>=5.4 in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pytorch-tabular) (6.0.1)\n",
      "Requirement already satisfied: matplotlib>3.1 in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pytorch-tabular) (3.7.1)\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pytorch-tabular) (8.1.3)\n",
      "Requirement already satisfied: einops<0.8.0,>=0.6.0 in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pytorch-tabular) (0.7.0)\n",
      "Requirement already satisfied: rich>=11.0.0 in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pytorch-tabular) (13.5.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib>3.1->pytorch-tabular) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib>3.1->pytorch-tabular) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib>3.1->pytorch-tabular) (4.39.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib>3.1->pytorch-tabular) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib>3.1->pytorch-tabular) (23.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib>3.1->pytorch-tabular) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib>3.1->pytorch-tabular) (3.0.9)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.12.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from omegaconf>=2.3.0->pytorch-tabular) (4.9.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pytorch-lightning<2.2.0,>=2.0.0->pytorch-tabular) (0.11.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from rich>=11.0.0->pytorch-tabular) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from rich>=11.0.0->pytorch-tabular) (2.15.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorboard!=2.5.0,>2.2.0->pytorch-tabular) (2.0.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorboard!=2.5.0,>2.2.0->pytorch-tabular) (1.60.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorboard!=2.5.0,>2.2.0->pytorch-tabular) (3.5.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorboard!=2.5.0,>2.2.0->pytorch-tabular) (70.0.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorboard!=2.5.0,>2.2.0->pytorch-tabular) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorboard!=2.5.0,>2.2.0->pytorch-tabular) (2.3.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm>=4.36->pytorch-tabnet) (0.4.6)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ipywidgets->pytorch-tabular) (0.1.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ipywidgets->pytorch-tabular) (8.14.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ipywidgets->pytorch-tabular) (5.9.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.11 in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ipywidgets->pytorch-tabular) (4.0.11)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.11 in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ipywidgets->pytorch-tabular) (3.0.11)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: requests in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning<2.2.0,>=2.0.0->pytorch-tabular) (2.31.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning<2.2.0,>=2.0.0->pytorch-tabular) (3.9.5)\n",
      "Requirement already satisfied: backcall in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ipython>=6.1.0->ipywidgets->pytorch-tabular) (0.2.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ipython>=6.1.0->ipywidgets->pytorch-tabular) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ipython>=6.1.0->ipywidgets->pytorch-tabular) (0.18.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ipython>=6.1.0->ipywidgets->pytorch-tabular) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ipython>=6.1.0->ipywidgets->pytorch-tabular) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ipython>=6.1.0->ipywidgets->pytorch-tabular) (3.0.39)\n",
      "Requirement already satisfied: stack-data in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ipython>=6.1.0->ipywidgets->pytorch-tabular) (0.6.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from markdown-it-py>=2.2.0->rich>=11.0.0->pytorch-tabular) (0.1.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<2.2.0,>=2.0.0->pytorch-tabular) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<2.2.0,>=2.0.0->pytorch-tabular) (22.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<2.2.0,>=2.0.0->pytorch-tabular) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<2.2.0,>=2.0.0->pytorch-tabular) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<2.2.0,>=2.0.0->pytorch-tabular) (1.9.4)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets->pytorch-tabular) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets->pytorch-tabular) (0.2.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning<2.2.0,>=2.0.0->pytorch-tabular) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning<2.2.0,>=2.0.0->pytorch-tabular) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning<2.2.0,>=2.0.0->pytorch-tabular) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning<2.2.0,>=2.0.0->pytorch-tabular) (2022.12.7)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets->pytorch-tabular) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets->pytorch-tabular) (2.2.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\andreab\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets->pytorch-tabular) (0.2.2)\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy pandas scikit-learn torch pytorch-tabnet pytorch-tabular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import e definizioni delle funzioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "# import torch\n",
    "# from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "# from pytorch_tabular.models import TabTransformerModel\n",
    "# from pytorch_tabular.config import ModelConfig, DataConfig, TrainerConfig\n",
    "# from pytorch_tabular import TabularModel\n",
    "import pickle\n",
    "\n",
    "# Funzioni di utilit√†\n",
    "def save_model(model, directory, filename):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    with open(os.path.join(directory, filename + '.pkl'), 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "def cross_val_score_with_preprocessing(model, X, y, cv, scaler_type, use_pca, n_components):\n",
    "    mse_scores, mae_scores, r2_scores = [], [], []\n",
    "    for train_idx, val_idx in KFold(n_splits=cv, shuffle=True, random_state=89).split(X):\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        # Scaling\n",
    "        if scaler_type == 'Standard':\n",
    "            scaler = StandardScaler().fit(X_train)\n",
    "        elif scaler_type == 'MinMax':\n",
    "            scaler = MinMaxScaler().fit(X_train)\n",
    "        X_train_scaled = scaler.transform(X_train)\n",
    "        X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "        # PCA\n",
    "        if use_pca:\n",
    "            pca = PCA(n_components=n_components).fit(X_train_scaled)\n",
    "            X_train_scaled = pca.transform(X_train_scaled)\n",
    "            X_val_scaled = pca.transform(X_val_scaled)\n",
    "        \n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_val_scaled)\n",
    "        mse_scores.append(mean_squared_error(y_val, y_pred))\n",
    "        mae_scores.append(mean_absolute_error(y_val, y_pred))\n",
    "        r2_scores.append(r2_score(y_val, y_pred))\n",
    "        \n",
    "    return np.mean(mse_scores), np.mean(mae_scores), np.mean(r2_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_state = 89\n",
    "\n",
    "# def save_model(model, directory, filename):\n",
    "#     if not os.path.exists(directory):\n",
    "#         os.makedirs(directory)\n",
    "#     joblib.dump(model, os.path.join(directory, f'{filename}.pkl'))\n",
    "\n",
    "# def preprocess(X, scaler_type='standard', use_pca=False, n_components=None):\n",
    "#     if scaler_type == 'standard':\n",
    "#         scaler = StandardScaler()\n",
    "#     elif scaler_type == 'minmax':\n",
    "#         scaler = MinMaxScaler()\n",
    "#     X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "#     pca = None\n",
    "#     if use_pca and n_components:\n",
    "#         pca = PCA(n_components=n_components, random_state=random_state)\n",
    "#         X_scaled = pca.fit_transform(X_scaled)\n",
    "\n",
    "#     return X_scaled, scaler, pca\n",
    "\n",
    "# def cross_val_score_with_preprocessing(model, X, y, cv=5, scaler_type='standard', use_pca=False, n_components=None):\n",
    "#     kf = KFold(n_splits=cv, shuffle=True, random_state=random_state)\n",
    "#     mse_scores = []\n",
    "#     mae_scores = []\n",
    "#     r2_scores = []\n",
    "\n",
    "#     for train_index, test_index in kf.split(X):\n",
    "#         X_train, X_test = X[train_index], X[test_index]\n",
    "#         y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "#         X_train_scaled, scaler, pca = preprocess(X_train, scaler_type=scaler_type, use_pca=use_pca, n_components=n_components)\n",
    "#         X_test_scaled = scaler.transform(X_test)\n",
    "#         if pca:\n",
    "#             X_test_scaled = pca.transform(X_test_scaled)\n",
    "\n",
    "#         model.fit(X_train_scaled, y_train)\n",
    "#         y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "#         mse_scores.append(mean_squared_error(y_test, y_pred))\n",
    "#         mae_scores.append(mean_absolute_error(y_test, y_pred))\n",
    "#         r2_scores.append(r2_score(y_test, y_pred))\n",
    "\n",
    "#     return np.mean(mse_scores), np.mean(mae_scores), np.mean(r2_scores)\n",
    "\n",
    "\n",
    "\n",
    "# def grid_search_cv_with_preprocessing(model, param_grid, X, y, cv=5, scaler_type='standard', use_pca=False, n_components=None):\n",
    "#     # Creazione del pipeline di preprocessing\n",
    "#     steps = []\n",
    "#     if scaler_type == 'standard':\n",
    "#         steps.append(('scaler', StandardScaler()))\n",
    "#     elif scaler_type == 'minmax':\n",
    "#         steps.append(('scaler', MinMaxScaler()))\n",
    "#     if use_pca:\n",
    "#         steps.append(('pca', PCA(n_components=n_components)))\n",
    "\n",
    "#     pipeline = Pipeline(steps + [('model', model)])\n",
    "    \n",
    "#     # Grid Search CV\n",
    "#     grid_search = GridSearchCV(pipeline, param_grid, cv=cv, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "#     grid_search.fit(X, y)\n",
    "    \n",
    "#     best_model = grid_search.best_estimator_\n",
    "#     best_params = grid_search.best_params_\n",
    "#     best_score = -grid_search.best_score_\n",
    "    \n",
    "#     return best_model, best_params, best_score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def save_performance(model_name, scaler_type, use_pca, mse, mae, r2, mse_cv, mae_cv, r2_cv, filename='model_performance.csv'):\n",
    "#     file_exists = os.path.isfile(filename)\n",
    "#     with open(filename, mode='a', newline='') as file:\n",
    "#         writer = csv.writer(file)\n",
    "#         if not file_exists:\n",
    "#             writer.writerow(['Model', 'Scaler', 'PCA', 'MSE', 'MAE', 'R2', 'CV_MSE', 'CV_MAE', 'CV_R2'])\n",
    "#         writer.writerow([model_name, scaler_type, use_pca, mse, mae, r2, mse_cv, mae_cv, r2_cv])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caricamento dei Dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurazione dei modelli\n",
    "preprocessing_options = {\n",
    "    'LR': {'scaler_type': ['Standard', 'MinMax'], 'use_pca': [True, False]},\n",
    "    'RF': {'scaler_type': ['Standard', 'MinMax'], 'use_pca': [True, False]},\n",
    "    'KNR': {'scaler_type': ['Standard', 'MinMax'], 'use_pca': [True, False]},\n",
    "    'SVR': {'scaler_type': ['Standard', 'MinMax'], 'use_pca': [True, False]},\n",
    "    'FFNN': {'scaler_type': ['Standard', 'MinMax'], 'use_pca': [True, False]},\n",
    "    'TabNet': {'scaler_type': ['Standard', 'MinMax'], 'use_pca': [False]},\n",
    "    'TabTransformer': {'scaler_type': ['Standard', 'MinMax'], 'use_pca': [False]}\n",
    "}\n",
    "\n",
    "# CSV zip folder's path\n",
    "csv_file_name = '../data.zip'\n",
    "# loading data from csv\n",
    "df = pd.read_csv(csv_file_name)\n",
    "\n",
    "X = df.drop('Year', axis=1)\n",
    "y = df['Year']\n",
    "\n",
    "# Divisione in training e validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=89)\n",
    "\n",
    "# Creazione degli scaler e PCA per ogni combinazione di preprocessing\n",
    "scalers = {}\n",
    "pcas = {}\n",
    "\n",
    "for model in preprocessing_options:\n",
    "    for scaler_type in preprocessing_options[model]['scaler_type']:\n",
    "        for use_pca in preprocessing_options[model]['use_pca']:\n",
    "            key = f\"{model}_{scaler_type}_{'PCA' if use_pca else 'NoPCA'}\"\n",
    "            if scaler_type == 'Standard':\n",
    "                scaler = StandardScaler().fit(X_train)\n",
    "            elif scaler_type == 'MinMax':\n",
    "                scaler = MinMaxScaler().fit(X_train)\n",
    "            scalers[key] = scaler\n",
    "            if use_pca:\n",
    "                pca = PCA(n_components=52).fit(scaler.transform(X_train))\n",
    "                pcas[key] = pca\n",
    "            else:\n",
    "                pcas[key] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funzioni di Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per addestrare i modelli\n",
    "def train_model(X_train_scaled, y_train, model_type):\n",
    "    if model_type == 'LR':\n",
    "        model = LinearRegression()\n",
    "    elif model_type == 'RF':\n",
    "        model = RandomForestRegressor(random_state=89)\n",
    "    elif model_type == 'KNR':\n",
    "        model = KNeighborsRegressor()\n",
    "    elif model_type == 'SVR':\n",
    "        model = SVR()\n",
    "    return model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Funzione per addestrare una rete neurale feed-forward\n",
    "def train_ffnn(X_train_scaled, y_train, input_dim):\n",
    "    class FFNN(torch.nn.Module):\n",
    "        def __init__(self, input_dim):\n",
    "            super(FFNN, self).__init__()\n",
    "            self.fc1 = torch.nn.Linear(input_dim, 128)\n",
    "            self.fc2 = torch.nn.Linear(128, 64)\n",
    "            self.fc3 = torch.nn.Linear(64, 1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = torch.relu(self.fc1(x))\n",
    "            x = torch.relu(self.fc2(x))\n",
    "            x = self.fc3(x)\n",
    "            return x\n",
    "\n",
    "    ffnn = FFNN(input_dim=input_dim)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(ffnn.parameters(), lr=0.001)\n",
    "\n",
    "    X_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    ffnn.train()\n",
    "    for epoch in range(100):  # Adjust the number of epochs as needed\n",
    "        optimizer.zero_grad()\n",
    "        outputs = ffnn(X_tensor)\n",
    "        loss = criterion(outputs, y_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return ffnn\n",
    "\n",
    "# Funzione per addestrare TabNet\n",
    "def train_tabnet(X_train_scaled, y_train, X_val_scaled, y_val):\n",
    "    tabnet = TabNetRegressor()\n",
    "    tabnet.fit(X_train_scaled, y_train, eval_set=[(X_val_scaled, y_val)], patience=10, max_epochs=100)\n",
    "    return tabnet\n",
    "\n",
    "# Funzione per addestrare TabTransformer\n",
    "def train_tabtransformer(df_train, df_val):\n",
    "    data_config = DataConfig(\n",
    "        target=['target_column'],\n",
    "        continuous_cols=df_train.columns.difference(['target_column']).tolist(),\n",
    "    )\n",
    "\n",
    "    model_config = ModelConfig(\n",
    "        task=\"regression\",\n",
    "        metrics=[\"mean_squared_error\", \"mean_absolute_error\", \"r2_score\"],\n",
    "        metrics_params=[{}, {}, {}]\n",
    "    )\n",
    "\n",
    "    trainer_config = TrainerConfig(\n",
    "        max_epochs=100,\n",
    "        gpus=0\n",
    "    )\n",
    "\n",
    "    tabular_model = TabularModel(\n",
    "        data_config=data_config,\n",
    "        model_config=model_config,\n",
    "        trainer_config=trainer_config\n",
    "    )\n",
    "\n",
    "    tabular_model.fit(train=df_train, validation=df_val)\n",
    "    return tabular_model\n",
    "\n",
    "# Preparazione del dizionario per salvare le performance\n",
    "performance_dict = {\n",
    "    'Model': [],\n",
    "    'Scaler': [],\n",
    "    'PCA': [],\n",
    "    'MSE_Val': [],\n",
    "    'MAE_Val': [],\n",
    "    'R2_Val': [],\n",
    "    'MSE_CV': [],\n",
    "    'MAE_CV': [],\n",
    "    'R2_CV': []\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing e salvataggio dei risultati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esegui il preprocessing specifico per ogni modello con Standard Scaler e Min-Max Scaler\n",
    "preprocessing_options = {\n",
    "    'LR': {'scaler_type': ['standard', 'minmax'], 'use_pca': [True, False], 'n_components': 52},\n",
    "    'RF': {'scaler_type': ['standard', 'minmax'], 'use_pca': [True, False], 'n_components': 52},\n",
    "    'KNR': {'scaler_type': ['standard', 'minmax'], 'use_pca': [True, False], 'n_components': 52},\n",
    "    'SVR': {'scaler_type': ['standard', 'minmax'], 'use_pca': [True, False], 'n_components': 52},\n",
    "    'FFNN': {'scaler_type': ['standard', 'minmax'], 'use_pca': [True, False], 'n_components': 52},\n",
    "    'TabNet': {'scaler_type': ['standard', 'minmax'], 'use_pca': [False], 'n_components': None},\n",
    "    'TabTransformer': {'scaler_type': ['standard', 'minmax'], 'use_pca': [False], 'n_components': None},\n",
    "}\n",
    "\n",
    "# Funzione per eseguire il preprocessing e salvare i risultati\n",
    "def preprocess_and_save(X_train, X_val, preprocessing_options):\n",
    "    scalers = {}\n",
    "    pcas = {}\n",
    "    for clfName, options in preprocessing_options.items():\n",
    "        for scaler_type in options['scaler_type']:\n",
    "            for use_pca in options['use_pca']:\n",
    "                key = f\"{clfName}_{scaler_type}_{'PCA' if use_pca else 'NoPCA'}\"\n",
    "                X_train_scaled, scaler, pca = preprocess(X_train, scaler_type=scaler_type, use_pca=use_pca, n_components=options['n_components'])\n",
    "                X_val_scaled = scaler.transform(X_val)\n",
    "                if pca:\n",
    "                    X_val_scaled = pca.transform(X_val_scaled)\n",
    "                scalers[key] = scaler\n",
    "                pcas[key] = pca\n",
    "                directory = os.path.join('models', key.lower())\n",
    "                save_model(scaler, directory, 'scaler')\n",
    "                if pca:\n",
    "                    save_model(pca, directory, 'pca')\n",
    "    return scalers, pcas\n",
    "\n",
    "# Esegui il preprocessing per tutte le combinazioni di scaler e PCA\n",
    "scalers, pcas = preprocess_and_save(X_train, X_val, preprocessing_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training trad models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All the models (DO NOT run this cell if you don't want your PC to crash or to explode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addestra i modelli tradizionali con tutte le combinazioni di scaler e PCA\n",
    "models = {}\n",
    "cv_performance = {}\n",
    "validation_performance = {}\n",
    "\n",
    "for clfName in ['LR', 'RF', 'KNR', 'SVR']:\n",
    "    for scaler_type in preprocessing_options[clfName]['scaler_type']:\n",
    "        for use_pca in preprocessing_options[clfName]['use_pca']:\n",
    "            key = f\"{clfName}_{scaler_type}_{'PCA' if use_pca else 'NoPCA'}\"\n",
    "            X_train_scaled = scalers[key].transform(X_train)\n",
    "            if pcas[key]:\n",
    "                X_train_scaled = pcas[key].transform(X_train_scaled)\n",
    "            \n",
    "            model = train_model(X_train_scaled, y_train, model_type=clfName)\n",
    "            models[key] = model\n",
    "            save_model(model, f'model_{key.lower()}')\n",
    "\n",
    "            # Valutazione su validation set\n",
    "            X_val_scaled = scalers[key].transform(X_val)\n",
    "            if pcas[key]:\n",
    "                X_val_scaled = pcas[key].transform(X_val_scaled)\n",
    "            y_pred = model.predict(X_val_scaled)\n",
    "            mse = mean_squared_error(y_val, y_pred)\n",
    "            mae = mean_absolute_error(y_val, y_pred)\n",
    "            r2 = r2_score(y_val, y_pred)\n",
    "            validation_performance[key] = {'mse': mse, 'mae': mae, 'r2': r2}\n",
    "            print(f\"Validation Performance of {clfName} with {scaler_type} scaler and PCA={use_pca}: MSE={mse}, MAE={mae}, R2={r2}\")\n",
    "\n",
    "            # Cross-validation\n",
    "            mse_cv, mae_cv, r2_cv = cross_val_score_with_preprocessing(\n",
    "                model, X.values, y, cv=5, scaler_type=scaler_type, use_pca=use_pca, n_components=preprocessing_options[clfName]['n_components']\n",
    "            )\n",
    "            cv_performance[key] = {'mse': mse_cv, 'mae': mae_cv, 'r2': r2_cv}\n",
    "            print(f\"Cross-Validation Performance of {clfName} with {scaler_type} scaler and PCA={use_pca}: MSE={mse_cv}, MAE={mae_cv}, R2={r2_cv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "performance = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Performance of LR with Standard scaler and PCA: MSE=98.2318351539259, MAE=7.392438758148582, R2=0.12050186127982243\n",
      "Validation Performance of LR with Standard scaler and NoPCA: MSE=85.58234834475938, MAE=6.668881118801984, R2=0.23375638907108864\n",
      "Validation Performance of LR with MinMax scaler and PCA: MSE=87.21013510994777, MAE=6.73281651718109, R2=0.21918234158461758\n",
      "Validation Performance of LR with MinMax scaler and NoPCA: MSE=85.58234834475938, MAE=6.668881118801991, R2=0.23375638907108864\n"
     ]
    }
   ],
   "source": [
    "clfName = 'LR'\n",
    "for scaler_type in preprocessing_options[clfName]['scaler_type']:\n",
    "    for use_pca in preprocessing_options[clfName]['use_pca']:\n",
    "        key = f\"{clfName}_{scaler_type}_{'PCA' if use_pca else 'NoPCA'}\"\n",
    "        X_train_scaled = scalers[key].transform(X_train)\n",
    "        if pcas[key]:\n",
    "            X_train_scaled = pcas[key].transform(X_train_scaled)\n",
    "\n",
    "        model = train_model(X_train_scaled, y_train, model_type=clfName)\n",
    "        directory = os.path.join('models', key.lower())\n",
    "        save_model(model, directory, 'model')\n",
    "\n",
    "        # Valutazione su validation set\n",
    "        X_val_scaled = scalers[key].transform(X_val)\n",
    "        if pcas[key]:\n",
    "            X_val_scaled = pcas[key].transform(X_val_scaled)\n",
    "        y_pred = model.predict(X_val_scaled)\n",
    "        mse = mean_squared_error(y_val, y_pred)\n",
    "        mae = mean_absolute_error(y_val, y_pred)\n",
    "        r2 = r2_score(y_val, y_pred)\n",
    "        print(f\"Validation Performance of {clfName} with {scaler_type} scaler and {'PCA' if use_pca else 'NoPCA'}: MSE={mse}, MAE={mae}, R2={r2}\")\n",
    "\n",
    "        # Salvare le performance\n",
    "        performance_dict['Model'].append(clfName)\n",
    "        performance_dict['Scaler'].append(scaler_type)\n",
    "        performance_dict['PCA'].append(use_pca)\n",
    "        performance_dict['MSE_Val'].append(mse)\n",
    "        performance_dict['MAE_Val'].append(mae)\n",
    "        performance_dict['R2_Val'].append(r2)\n",
    "        performance_dict['MSE_CV'].append(None)  # No cross-validation for Linear Regression\n",
    "        performance_dict['MAE_CV'].append(None)\n",
    "        performance_dict['R2_CV'].append(None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Performance of RF with Standard scaler and PCA: MSE=92.01214625373923, MAE=7.210540541101935, R2=0.17618854169825393\n",
      "Cross-Validation Performance of RF with Standard scaler and PCA: MSE=90.85271300574892, MAE=7.169012839722981, R2=0.17553096742065438\n",
      "Validation Performance of RF with Standard scaler and NoPCA: MSE=79.40827703444918, MAE=6.436896507055287, R2=0.2890346419636943\n",
      "Cross-Validation Performance of RF with Standard scaler and NoPCA: MSE=78.4878556946283, MAE=6.400234070255443, R2=0.28774069750333975\n",
      "Validation Performance of RF with MinMax scaler and PCA: MSE=81.65413718735886, MAE=6.573940451406099, R2=0.26892680399838764\n",
      "Cross-Validation Performance of RF with MinMax scaler and PCA: MSE=80.79298888304402, MAE=6.526083350469959, R2=0.2668224632088366\n",
      "Validation Performance of RF with MinMax scaler and NoPCA: MSE=79.42058209449787, MAE=6.438306572816495, R2=0.28892447118868336\n",
      "Cross-Validation Performance of RF with MinMax scaler and NoPCA: MSE=78.52371315641918, MAE=6.401475427778328, R2=0.2874174746901928\n"
     ]
    }
   ],
   "source": [
    "clfName = 'RF'\n",
    "for scaler_type in preprocessing_options[clfName]['scaler_type']:\n",
    "    for use_pca in preprocessing_options[clfName]['use_pca']:\n",
    "        key = f\"{clfName}_{scaler_type}_{'PCA' if use_pca else 'NoPCA'}\"\n",
    "        X_train_scaled = scalers[key].transform(X_train)\n",
    "        if pcas[key]:\n",
    "            X_train_scaled = pcas[key].transform(X_train_scaled)\n",
    "\n",
    "        model = train_model(X_train_scaled, y_train, model_type=clfName)\n",
    "        directory = os.path.join('models', key.lower())\n",
    "        save_model(model, directory, 'model')\n",
    "\n",
    "        # Valutazione su validation set\n",
    "        X_val_scaled = scalers[key].transform(X_val)\n",
    "        if pcas[key]:\n",
    "            X_val_scaled = pcas[key].transform(X_val_scaled)\n",
    "        y_pred = model.predict(X_val_scaled)\n",
    "        mse = mean_squared_error(y_val, y_pred)\n",
    "        mae = mean_absolute_error(y_val, y_pred)\n",
    "        r2 = r2_score(y_val, y_pred)\n",
    "        print(f\"Validation Performance of {clfName} with {scaler_type} scaler and {'PCA' if use_pca else 'NoPCA'}: MSE={mse}, MAE={mae}, R2={r2}\")\n",
    "\n",
    "        # Cross-validation\n",
    "        mse_cv, mae_cv, r2_cv = cross_val_score_with_preprocessing(\n",
    "            model, X.values, y.values, cv=5, scaler_type=scaler_type, use_pca=use_pca, n_components=52\n",
    "        )\n",
    "        print(f\"Cross-Validation Performance of {clfName} with {scaler_type} scaler and {'PCA' if use_pca else 'NoPCA'}: MSE={mse_cv}, MAE={mae_cv}, R2={r2_cv}\")\n",
    "\n",
    "        # Salvare le performance\n",
    "        performance_dict['Model'].append(clfName)\n",
    "        performance_dict['Scaler'].append(scaler_type)\n",
    "        performance_dict['PCA'].append(use_pca)\n",
    "        performance_dict['MSE_Val'].append(mse)\n",
    "        performance_dict['MAE_Val'].append(mae)\n",
    "        performance_dict['R2_Val'].append(r2)\n",
    "        performance_dict['MSE_CV'].append(mse_cv)\n",
    "        performance_dict['MAE_CV'].append(mae_cv)\n",
    "        performance_dict['R2_CV'].append(r2_cv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training KNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gabriele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "found 0 physical cores < 1\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"c:\\Users\\Gabriele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 282, in _count_physical_cores\n",
      "    raise ValueError(f\"found {cpu_count_physical} physical cores < 1\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Performance of KNR with Standard scaler and PCA: MSE=95.77045226529194, MAE=7.149356597600872, R2=0.14253933686845544\n",
      "Cross-Validation Performance of KNR with Standard scaler and PCA: MSE=94.88380767324279, MAE=7.103034797263805, R2=0.1388833328796148\n",
      "Validation Performance of KNR with Standard scaler and NoPCA: MSE=89.67204282740155, MAE=6.853474769505305, R2=0.19714016705119275\n",
      "Cross-Validation Performance of KNR with Standard scaler and NoPCA: MSE=88.92059514226231, MAE=6.822715177951821, R2=0.19298139456120558\n",
      "Validation Performance of KNR with MinMax scaler and PCA: MSE=85.17326063249726, MAE=6.556490532368397, R2=0.23741907012498586\n",
      "Cross-Validation Performance of KNR with MinMax scaler and PCA: MSE=84.32230474868643, MAE=6.529097650441163, R2=0.23475225724794607\n",
      "Validation Performance of KNR with MinMax scaler and NoPCA: MSE=84.66267790225041, MAE=6.556375532864084, R2=0.24199046554085202\n",
      "Cross-Validation Performance of KNR with MinMax scaler and NoPCA: MSE=83.89033330028748, MAE=6.532507980569051, R2=0.2386649377271592\n"
     ]
    }
   ],
   "source": [
    "clfName = 'KNR'\n",
    "for scaler_type in preprocessing_options[clfName]['scaler_type']:\n",
    "    for use_pca in preprocessing_options[clfName]['use_pca']:\n",
    "        key = f\"{clfName}_{scaler_type}_{'PCA' if use_pca else 'NoPCA'}\"\n",
    "        X_train_scaled = scalers[key].transform(X_train)\n",
    "        if pcas[key]:\n",
    "            X_train_scaled = pcas[key].transform(X_train_scaled)\n",
    "\n",
    "        model = train_model(X_train_scaled, y_train, model_type=clfName)\n",
    "        directory = os.path.join('models', key.lower())\n",
    "        save_model(model, directory, 'model')\n",
    "\n",
    "        # Valutazione su validation set\n",
    "        X_val_scaled = scalers[key].transform(X_val)\n",
    "        if pcas[key]:\n",
    "            X_val_scaled = pcas[key].transform(X_val_scaled)\n",
    "        y_pred = model.predict(X_val_scaled)\n",
    "        mse = mean_squared_error(y_val, y_pred)\n",
    "        mae = mean_absolute_error(y_val, y_pred)\n",
    "        r2 = r2_score(y_val, y_pred)\n",
    "        print(f\"Validation Performance of {clfName} with {scaler_type} scaler and {'PCA' if use_pca else 'NoPCA'}: MSE={mse}, MAE={mae}, R2={r2}\")\n",
    "\n",
    "        # Cross-validation\n",
    "        mse_cv, mae_cv, r2_cv = cross_val_score_with_preprocessing(\n",
    "            model, X.values, y.values, cv=5, scaler_type=scaler_type, use_pca=use_pca, n_components=52\n",
    "        )\n",
    "        print(f\"Cross-Validation Performance of {clfName} with {scaler_type} scaler and {'PCA' if use_pca else 'NoPCA'}: MSE={mse_cv}, MAE={mae_cv}, R2={r2_cv}\")\n",
    "\n",
    "        # Salvare le performance\n",
    "        performance_dict['Model'].append(clfName)\n",
    "        performance_dict['Scaler'].append(scaler_type)\n",
    "        performance_dict['PCA'].append(use_pca)\n",
    "        performance_dict['MSE_Val'].append(mse)\n",
    "        performance_dict['MAE_Val'].append(mae)\n",
    "        performance_dict['R2_Val'].append(r2)\n",
    "        performance_dict['MSE_CV'].append(mse_cv)\n",
    "        performance_dict['MAE_CV'].append(mae_cv)\n",
    "        performance_dict['R2_CV'].append(r2_cv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocessing_options' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m clfName \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSVR\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m scaler_type \u001b[38;5;129;01min\u001b[39;00m \u001b[43mpreprocessing_options\u001b[49m[clfName][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscaler_type\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m use_pca \u001b[38;5;129;01min\u001b[39;00m preprocessing_options[clfName][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_pca\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m      4\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclfName\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscaler_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPCA\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39muse_pca\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNoPCA\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'preprocessing_options' is not defined"
     ]
    }
   ],
   "source": [
    "clfName = 'SVR'\n",
    "for scaler_type in preprocessing_options[clfName]['scaler_type']:\n",
    "    for use_pca in preprocessing_options[clfName]['use_pca']:\n",
    "        key = f\"{clfName}_{scaler_type}_{'PCA' if use_pca else 'NoPCA'}\"\n",
    "        X_train_scaled = scalers[key].transform(X_train)\n",
    "        if pcas[key]:\n",
    "            X_train_scaled = pcas[key].transform(X_train_scaled)\n",
    "\n",
    "        model = train_model(X_train_scaled, y_train, model_type=clfName)\n",
    "        directory = os.path.join('models', key.lower())\n",
    "        save_model(model, directory, 'model')\n",
    "\n",
    "        # Valutazione su validation set\n",
    "        X_val_scaled = scalers[key].transform(X_val)\n",
    "        if pcas[key]:\n",
    "            X_val_scaled = pcas[key].transform(X_val_scaled)\n",
    "        y_pred = model.predict(X_val_scaled)\n",
    "        mse = mean_squared_error(y_val, y_pred)\n",
    "        mae = mean_absolute_error(y_val, y_pred)\n",
    "        r2 = r2_score(y_val, y_pred)\n",
    "        print(f\"Validation Performance of {clfName} with {scaler_type} scaler and {'PCA' if use_pca else 'NoPCA'}: MSE={mse}, MAE={mae}, R2={r2}\")\n",
    "\n",
    "        # Cross-validation\n",
    "        mse_cv, mae_cv, r2_cv = cross_val_score_with_preprocessing(\n",
    "            model, X.values, y.values, cv=5, scaler_type=scaler_type, use_pca=use_pca, n_components=52\n",
    "        )\n",
    "        print(f\"Cross-Validation Performance of {clfName} with {scaler_type} scaler and {'PCA' if use_pca else 'NoPCA'}: MSE={mse_cv}, MAE={mae_cv}, R2={r2_cv}\")\n",
    "\n",
    "        # Salvare le performance\n",
    "        performance_dict['Model'].append(clfName)\n",
    "        performance_dict['Scaler'].append(scaler_type)\n",
    "        performance_dict['PCA'].append(use_pca)\n",
    "        performance_dict['MSE_Val'].append(mse)\n",
    "        performance_dict['MAE_Val'].append(mae)\n",
    "        performance_dict['R2_Val'].append(r2)\n",
    "        performance_dict['MSE_CV'].append(mse_cv)\n",
    "        performance_dict['MAE_CV'].append(mae_cv)\n",
    "        performance_dict['R2_CV'].append(r2_cv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m csv_file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data.zip\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# loading data from csv\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(csv_file_name)\n\u001b[0;32m      6\u001b[0m X \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      7\u001b[0m y \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# CSV zip folder's path\n",
    "csv_file_name = '../data.zip'\n",
    "# loading data from csv\n",
    "df = pd.read_csv(csv_file_name)\n",
    "\n",
    "X = df.drop('Year', axis=1)\n",
    "y = df['Year']\n",
    "\n",
    "# Divisione in training e validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=89)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training FFNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't pickle local object 'train_ffnn.<locals>.FFNN'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m model \u001b[38;5;241m=\u001b[39m train_ffnn(X_train_scaled, y_train, input_dim\u001b[38;5;241m=\u001b[39mX_train_scaled\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     11\u001b[0m directory \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m'\u001b[39m, key\u001b[38;5;241m.\u001b[39mlower())\n\u001b[1;32m---> 12\u001b[0m \u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Valutazione su validation set\u001b[39;00m\n\u001b[0;32m     15\u001b[0m X_val_scaled \u001b[38;5;241m=\u001b[39m scalers[key]\u001b[38;5;241m.\u001b[39mtransform(X_val)\n",
      "Cell \u001b[1;32mIn[14], line 24\u001b[0m, in \u001b[0;36msave_model\u001b[1;34m(model, directory, filename)\u001b[0m\n\u001b[0;32m     22\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(directory)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, filename \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m---> 24\u001b[0m     \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: Can't pickle local object 'train_ffnn.<locals>.FFNN'"
     ]
    }
   ],
   "source": [
    "clfName = 'FFNN'\n",
    "input_dim = X_train.shape[1]\n",
    "for scaler_type in preprocessing_options[clfName]['scaler_type']:\n",
    "    for use_pca in preprocessing_options[clfName]['use_pca']:\n",
    "        key = f\"{clfName}_{scaler_type}_{'PCA' if use_pca else 'NoPCA'}\"\n",
    "        X_train_scaled = scalers[key].transform(X_train)\n",
    "        if pcas[key]:\n",
    "            X_train_scaled = pcas[key].transform(X_train_scaled)\n",
    "\n",
    "        model = train_ffnn(X_train_scaled, y_train, input_dim=X_train_scaled.shape[1])\n",
    "        directory = os.path.join('models', key.lower())\n",
    "        save_model(model, directory, 'model')\n",
    "\n",
    "        # Valutazione su validation set\n",
    "        X_val_scaled = scalers[key].transform(X_val)\n",
    "        if pcas[key]:\n",
    "            X_val_scaled = pcas[key].transform(X_val_scaled)\n",
    "        X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(X_val_tensor).numpy()\n",
    "        mse = mean_squared_error(y_val, y_pred)\n",
    "        mae = mean_absolute_error(y_val, y_pred)\n",
    "        r2 = r2_score(y_val, y_pred)\n",
    "        print(f\"Validation Performance of {clfName} with {scaler_type} scaler and {'PCA' if use_pca else 'NoPCA'}: MSE={mse}, MAE={mae}, R2={r2}\")\n",
    "\n",
    "        # Cross-validation\n",
    "        mse_cv, mae_cv, r2_cv = cross_val_score_with_preprocessing(\n",
    "            model, X.values, y.values, cv=5, scaler_type=scaler_type, use_pca=use_pca, n_components=52\n",
    "        )\n",
    "        print(f\"Cross-Validation Performance of {clfName} with {scaler_type} scaler and {'PCA' if use_pca else 'NoPCA'}: MSE={mse_cv}, MAE={mae_cv}, R2={r2_cv}\")\n",
    "\n",
    "        # Salvare le performance\n",
    "        performance_dict['Model'].append(clfName)\n",
    "        performance_dict['Scaler'].append(scaler_type)\n",
    "        performance_dict['PCA'].append(use_pca)\n",
    "        performance_dict['MSE_Val'].append(mse)\n",
    "        performance_dict['MAE_Val'].append(mae)\n",
    "        performance_dict['R2_Val'].append(r2)\n",
    "        performance_dict['MSE_CV'].append(mse_cv)\n",
    "        performance_dict['MAE_CV'].append(mae_cv)\n",
    "        performance_dict['R2_CV'].append(r2_cv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training TabNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gabriele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Targets should be 2D : (n_samples, n_regression) but y_train.shape=(201740,) given.\nUse reshape(-1, 1) for single regression.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m X_train_scaled \u001b[38;5;241m=\u001b[39m scalers[key]\u001b[38;5;241m.\u001b[39mtransform(X_train)\n\u001b[0;32m      6\u001b[0m X_val_scaled \u001b[38;5;241m=\u001b[39m scalers[key]\u001b[38;5;241m.\u001b[39mtransform(X_val)\n\u001b[1;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_tabnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m directory \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m'\u001b[39m, key\u001b[38;5;241m.\u001b[39mlower())\n\u001b[0;32m     10\u001b[0m save_model(model, directory, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[16], line 48\u001b[0m, in \u001b[0;36mtrain_tabnet\u001b[1;34m(X_train_scaled, y_train, X_val_scaled, y_val)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_tabnet\u001b[39m(X_train_scaled, y_train, X_val_scaled, y_val):\n\u001b[0;32m     47\u001b[0m     tabnet \u001b[38;5;241m=\u001b[39m TabNetRegressor()\n\u001b[1;32m---> 48\u001b[0m     \u001b[43mtabnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tabnet\n",
      "File \u001b[1;32mc:\\Users\\Gabriele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:220\u001b[0m, in \u001b[0;36mTabModel.fit\u001b[1;34m(self, X_train, y_train, eval_set, eval_name, eval_metric, loss_fn, weights, max_epochs, patience, batch_size, virtual_batch_size, num_workers, drop_last, callbacks, pin_memory, from_unsupervised, warm_start, augmentations, compute_importance)\u001b[0m\n\u001b[0;32m    217\u001b[0m check_input(X_train)\n\u001b[0;32m    218\u001b[0m check_warm_start(warm_start, from_unsupervised)\n\u001b[1;32m--> 220\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_fit_params\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;66;03m# Validate and reformat eval set depending on training data\u001b[39;00m\n\u001b[0;32m    228\u001b[0m eval_names, eval_set \u001b[38;5;241m=\u001b[39m validate_eval_set(eval_set, eval_name, X_train, y_train)\n",
      "File \u001b[1;32mc:\\Users\\Gabriele\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_tabnet\\tab_model.py:141\u001b[0m, in \u001b[0;36mTabNetRegressor.update_fit_params\u001b[1;34m(self, X_train, y_train, eval_set, weights)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_train\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    138\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTargets should be 2D : (n_samples, n_regression) \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \\\n\u001b[0;32m    139\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut y_train.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_train\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m given.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \\\n\u001b[0;32m    140\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse reshape(-1, 1) for single regression.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 141\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_dim \u001b[38;5;241m=\u001b[39m y_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreds_mapper \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Targets should be 2D : (n_samples, n_regression) but y_train.shape=(201740,) given.\nUse reshape(-1, 1) for single regression."
     ]
    }
   ],
   "source": [
    "clfName = 'TabNet'\n",
    "for scaler_type in preprocessing_options[clfName]['scaler_type']:\n",
    "    for use_pca in preprocessing_options[clfName]['use_pca']:\n",
    "        key = f\"{clfName}_{scaler_type}_{'PCA' if use_pca else 'NoPCA'}\"\n",
    "        X_train_scaled = scalers[key].transform(X_train)\n",
    "        X_val_scaled = scalers[key].transform(X_val)\n",
    "\n",
    "        model = train_tabnet(X_train_scaled, y_train, X_val_scaled, y_val)\n",
    "        directory = os.path.join('models', key.lower())\n",
    "        save_model(model, directory, 'model')\n",
    "\n",
    "        # Valutazione su validation set\n",
    "        y_pred = model.predict(X_val_scaled)\n",
    "        mse = mean_squared_error(y_val, y_pred)\n",
    "        mae = mean_absolute_error(y_val, y_pred)\n",
    "        r2 = r2_score(y_val, y_pred)\n",
    "        print(f\"Validation Performance of {clfName} with {scaler_type} scaler and {'PCA' if use_pca else 'NoPCA'}: MSE={mse}, MAE={mae}, R2={r2}\")\n",
    "\n",
    "        # Cross-validation\n",
    "        mse_cv, mae_cv, r2_cv = cross_val_score_with_preprocessing(\n",
    "            model, X.values, y.values, cv=5, scaler_type=scaler_type, use_pca=use_pca, n_components=52\n",
    "        )\n",
    "        print(f\"Cross-Validation Performance of {clfName} with {scaler_type} scaler and {'PCA' if use_pca else 'NoPCA'}: MSE={mse_cv}, MAE={mae_cv}, R2={r2_cv}\")\n",
    "\n",
    "        # Salvare le performance\n",
    "        performance_dict['Model'].append(clfName)\n",
    "        performance_dict['Scaler'].append(scaler_type)\n",
    "        performance_dict['PCA'].append(use_pca)\n",
    "        performance_dict['MSE_Val'].append(mse)\n",
    "        performance_dict['MAE_Val'].append(mae)\n",
    "        performance_dict['R2_Val'].append(r2)\n",
    "        performance_dict['MSE_CV'].append(mse_cv)\n",
    "        performance_dict['MAE_CV'].append(mae_cv)\n",
    "        performance_dict['R2_CV'].append(r2_cv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training TabTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TrainerConfig.__init__() got an unexpected keyword argument 'gpus'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m df_val_scaled \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(X_val_scaled, columns\u001b[38;5;241m=\u001b[39mX_val\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m     11\u001b[0m df_val_scaled[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_column\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m y_val\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m---> 13\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_tabtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_val_scaled\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m directory \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m'\u001b[39m, key\u001b[38;5;241m.\u001b[39mlower())\n\u001b[0;32m     15\u001b[0m save_model(model, directory, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[16], line 64\u001b[0m, in \u001b[0;36mtrain_tabtransformer\u001b[1;34m(df_train, df_val)\u001b[0m\n\u001b[0;32m     53\u001b[0m data_config \u001b[38;5;241m=\u001b[39m DataConfig(\n\u001b[0;32m     54\u001b[0m     target\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_column\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     55\u001b[0m     continuous_cols\u001b[38;5;241m=\u001b[39mdf_train\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mdifference([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_column\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[0;32m     56\u001b[0m )\n\u001b[0;32m     58\u001b[0m model_config \u001b[38;5;241m=\u001b[39m ModelConfig(\n\u001b[0;32m     59\u001b[0m     task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregression\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     60\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean_squared_error\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean_absolute_error\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr2_score\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     61\u001b[0m     metrics_params\u001b[38;5;241m=\u001b[39m[{}, {}, {}]\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 64\u001b[0m trainer_config \u001b[38;5;241m=\u001b[39m \u001b[43mTrainerConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[0;32m     67\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m tabular_model \u001b[38;5;241m=\u001b[39m TabularModel(\n\u001b[0;32m     70\u001b[0m     data_config\u001b[38;5;241m=\u001b[39mdata_config,\n\u001b[0;32m     71\u001b[0m     model_config\u001b[38;5;241m=\u001b[39mmodel_config,\n\u001b[0;32m     72\u001b[0m     trainer_config\u001b[38;5;241m=\u001b[39mtrainer_config\n\u001b[0;32m     73\u001b[0m )\n\u001b[0;32m     75\u001b[0m tabular_model\u001b[38;5;241m.\u001b[39mfit(train\u001b[38;5;241m=\u001b[39mdf_train, validation\u001b[38;5;241m=\u001b[39mdf_val)\n",
      "\u001b[1;31mTypeError\u001b[0m: TrainerConfig.__init__() got an unexpected keyword argument 'gpus'"
     ]
    }
   ],
   "source": [
    "clfName = 'TabTransformer'\n",
    "for scaler_type in preprocessing_options[clfName]['scaler_type']:\n",
    "    for use_pca in preprocessing_options[clfName]['use_pca']:\n",
    "        key = f\"{clfName}_{scaler_type}_{'PCA' if use_pca else 'NoPCA'}\"\n",
    "        X_train_scaled = scalers[key].transform(X_train)\n",
    "        X_val_scaled = scalers[key].transform(X_val)\n",
    "\n",
    "        df_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "        df_train_scaled['target_column'] = y_train.values\n",
    "        df_val_scaled = pd.DataFrame(X_val_scaled, columns=X_val.columns)\n",
    "        df_val_scaled['target_column'] = y_val.values\n",
    "\n",
    "        model = train_tabtransformer(df_train_scaled, df_val_scaled)\n",
    "        directory = os.path.join('models', key.lower())\n",
    "        save_model(model, directory, 'model')\n",
    "\n",
    "        # Valutazione su validation set\n",
    "        y_pred = model.predict(X_val_scaled)\n",
    "        mse = mean_squared_error(y_val, y_pred)\n",
    "        mae = mean_absolute_error(y_val, y_pred)\n",
    "        r2 = r2_score(y_val, y_pred)\n",
    "        print(f\"Validation Performance of {clfName} with {scaler_type} scaler and {'PCA' if use_pca else 'NoPCA'}: MSE={mse}, MAE={mae}, R2={r2}\")\n",
    "\n",
    "        # Cross-validation\n",
    "        mse_cv, mae_cv, r2_cv = cross_val_score_with_preprocessing(\n",
    "            model, X.values, y.values, cv=5, scaler_type=scaler_type, use_pca=use_pca, n_components=52\n",
    "        )\n",
    "        print(f\"Cross-Validation Performance of {clfName} with {scaler_type} scaler and {'PCA' if use_pca else 'NoPCA'}: MSE={mse_cv}, MAE={mae_cv}, R2={r2_cv}\")\n",
    "\n",
    "        # Salvare le performance\n",
    "        performance_dict['Model'].append(clfName)\n",
    "        performance_dict['Scaler'].append(scaler_type)\n",
    "        performance_dict['PCA'].append(use_pca)\n",
    "        performance_dict['MSE_Val'].append(mse)\n",
    "        performance_dict['MAE_Val'].append(mae)\n",
    "        performance_dict['R2_Val'].append(r2)\n",
    "        performance_dict['MSE_CV'].append(mse_cv)\n",
    "        performance_dict['MAE_CV'].append(mae_cv)\n",
    "        performance_dict['R2_CV'].append(r2_cv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All the models (DO NOT run this cell if you don't want your PC to crash or to explode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valutazione dei modelli con tutte le combinazioni di scaler e PCA\n",
    "# performance = {}\n",
    "\n",
    "# Valutazione modelli tradizionali\n",
    "for clfName in ['LR', 'RF', 'KNR', 'SVR']:\n",
    "    for scaler_type in preprocessing_options[clfName]['scaler_type']:\n",
    "        for use_pca in preprocessing_options[clfName]['use_pca']:\n",
    "            key = f\"{clfName}_{scaler_type}_{'PCA' if use_pca else 'NoPCA'}\"\n",
    "            X_val_scaled = scalers[key].transform(X_val)\n",
    "            if pcas[key]:\n",
    "                X_val_scaled = pcas[key].transform(X_val_scaled)\n",
    "            y_pred = models[key].predict(X_val_scaled)\n",
    "            mse = mean_squared_error(y_val, y_pred)\n",
    "            mae = mean_absolute_error(y_val, y_pred)\n",
    "            r2 = r2_score(y_val, y_pred)\n",
    "            performance[key] = {'mse': mse, 'mae': mae, 'r2': r2}\n",
    "            print(f\"Performance of {clfName} with {scaler_type} scaler and PCA={use_pca}: MSE={mse}, MAE={mae}, R2={r2}\")\n",
    "\n",
    "# Valutazione FFNN\n",
    "for scaler_type in preprocessing_options['FFNN']['scaler_type']:\n",
    "    for use_pca in preprocessing_options['FFNN']['use_pca']:\n",
    "        key = f\"FFNN_{scaler_type}_{'PCA' if use_pca else 'NoPCA'}\"\n",
    "        X_val_scaled = scalers[key].transform(X_val)\n",
    "        if pcas[key]:\n",
    "            X_val_scaled = pcas[key].transform(X_val_scaled)\n",
    "        X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32)\n",
    "        y_pred_tensor = models[key](X_val_tensor).detach().numpy().squeeze()\n",
    "        mse = mean_squared_error(y_val, y_pred_tensor)\n",
    "        mae = mean_absolute_error(y_val, y_pred_tensor)\n",
    "        r2 = r2_score(y_val, y_pred_tensor)\n",
    "        performance[key] = {'mse': mse, 'mae': mae, 'r2': r2}\n",
    "        print(f\"Performance of FFNN with {scaler_type} scaler and PCA={use_pca}: MSE={mse}, MAE={mae}, R2={r2}\")\n",
    "\n",
    "# Valutazione TabNet\n",
    "for scaler_type in preprocessing_options['TabNet']['scaler_type']:\n",
    "    key = f\"TabNet_{scaler_type}_NoPCA\"\n",
    "    X_val_scaled = scalers[key].transform(X_val)\n",
    "    y_pred = models[key].predict(X_val_scaled)\n",
    "    mse = mean_squared_error(y_val, y_pred)\n",
    "    mae = mean_absolute_error(y_val, y_pred)\n",
    "    r2 = r2_score(y_val, y_pred)\n",
    "    performance[key] = {'mse': mse, 'mae': mae, 'r2': r2}\n",
    "    print(f\"Performance of TabNet with {scaler_type} scaler: MSE={mse}, MAE={mae}, R2={r2}\")\n",
    "\n",
    "# Valutazione TabTransformer\n",
    "for scaler_type in preprocessing_options['TabTransformer']['scaler_type']:\n",
    "    key = f\"TabTransformer_{scaler_type}_NoPCA\"\n",
    "    y_pred = models[key].predict(df_val).squeeze()\n",
    "    mse = mean_squared_error(y_val, y_pred)\n",
    "    mae = mean_absolute_error(y_val, y_pred)\n",
    "    r2 = r2_score(y_val, y_pred)\n",
    "    performance[key] = {'mse': mse, 'mae': mae, 'r2': r2}\n",
    "    print(f\"Performance of TabTransformer with {scaler_type} scaler: MSE={mse}, MAE={mae}, R2={r2}\")\n",
    "\n",
    "# Salva le performance dei modelli\n",
    "with open('performance.txt', 'w') as f:\n",
    "    for clfName, metrics in performance.items():\n",
    "        f.write(f\"{clfName}: MSE={metrics['mse']}, MAE={metrics['mae']}, R2={metrics['r2']}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of LR with standard scaler and PCA=True: MSE=98.2318351539259, MAE=7.392438758148582, R2=0.12050186127982243\n",
      "Performance of LR with standard scaler and PCA=False: MSE=85.58234834475938, MAE=6.668881118801984, R2=0.23375638907108864\n",
      "Performance of LR with minmax scaler and PCA=True: MSE=87.21013510994777, MAE=6.73281651718109, R2=0.21918234158461758\n",
      "Performance of LR with minmax scaler and PCA=False: MSE=85.58234834475938, MAE=6.668881118801991, R2=0.23375638907108864\n"
     ]
    }
   ],
   "source": [
    "# Valutazione dei modelli con tutte le combinazioni di scaler e PCA\n",
    "# performance = {}\n",
    "\n",
    "# Valutazione modelli tradizionali\n",
    "for clfName in ['LR']:\n",
    "    for scaler_type in preprocessing_options[clfName]['scaler_type']:\n",
    "        for use_pca in preprocessing_options[clfName]['use_pca']:\n",
    "            key = f\"{clfName}_{scaler_type}_{'PCA' if use_pca else 'NoPCA'}\"\n",
    "            X_val_scaled = scalers[key].transform(X_val)\n",
    "            if pcas[key]:\n",
    "                X_val_scaled = pcas[key].transform(X_val_scaled)\n",
    "            y_pred = models[key].predict(X_val_scaled)\n",
    "            mse = mean_squared_error(y_val, y_pred)\n",
    "            mae = mean_absolute_error(y_val, y_pred)\n",
    "            r2 = r2_score(y_val, y_pred)\n",
    "            performance[key] = {'mse': mse, 'mae': mae, 'r2': r2}\n",
    "            print(f\"Performance of {clfName} with {scaler_type} scaler and PCA={use_pca}: MSE={mse}, MAE={mae}, R2={r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation RF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of RF with standard scaler and PCA=True: MSE=92.01214625373923, MAE=7.210540541101935, R2=0.17618854169825393\n",
      "Performance of RF with standard scaler and PCA=False: MSE=79.40827703444918, MAE=6.436896507055287, R2=0.2890346419636943\n",
      "Performance of RF with minmax scaler and PCA=True: MSE=81.65413718735886, MAE=6.573940451406099, R2=0.26892680399838764\n",
      "Performance of RF with minmax scaler and PCA=False: MSE=79.42058209449787, MAE=6.438306572816495, R2=0.28892447118868336\n"
     ]
    }
   ],
   "source": [
    "# Valutazione dei modelli con tutte le combinazioni di scaler e PCA\n",
    "performance = {}\n",
    "\n",
    "# Valutazione modelli tradizionali\n",
    "for clfName in ['RF']:\n",
    "    for scaler_type in preprocessing_options[clfName]['scaler_type']:\n",
    "        for use_pca in preprocessing_options[clfName]['use_pca']:\n",
    "            key = f\"{clfName}_{scaler_type}_{'PCA' if use_pca else 'NoPCA'}\"\n",
    "            X_val_scaled = scalers[key].transform(X_val)\n",
    "            if pcas[key]:\n",
    "                X_val_scaled = pcas[key].transform(X_val_scaled)\n",
    "            y_pred = models[key].predict(X_val_scaled)\n",
    "            mse = mean_squared_error(y_val, y_pred)\n",
    "            mae = mean_absolute_error(y_val, y_pred)\n",
    "            r2 = r2_score(y_val, y_pred)\n",
    "            performance[key] = {'mse': mse, 'mae': mae, 'r2': r2}\n",
    "            print(f\"Performance of {clfName} with {scaler_type} scaler and PCA={use_pca}: MSE={mse}, MAE={mae}, R2={r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation KNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of KNR with standard scaler and PCA=True: MSE=95.77045226529194, MAE=7.149356597600872, R2=0.14253933686845544\n",
      "Performance of KNR with standard scaler and PCA=False: MSE=89.67204282740155, MAE=6.853474769505305, R2=0.19714016705119275\n",
      "Performance of KNR with minmax scaler and PCA=True: MSE=85.17326063249726, MAE=6.556490532368397, R2=0.23741907012498586\n",
      "Performance of KNR with minmax scaler and PCA=False: MSE=84.66267790225041, MAE=6.556375532864084, R2=0.24199046554085202\n"
     ]
    }
   ],
   "source": [
    "# Valutazione modelli tradizionali\n",
    "for clfName in ['KNR']:\n",
    "    for scaler_type in preprocessing_options[clfName]['scaler_type']:\n",
    "        for use_pca in preprocessing_options[clfName]['use_pca']:\n",
    "            key = f\"{clfName}_{scaler_type}_{'PCA' if use_pca else 'NoPCA'}\"\n",
    "            X_val_scaled = scalers[key].transform(X_val)\n",
    "            if pcas[key]:\n",
    "                X_val_scaled = pcas[key].transform(X_val_scaled)\n",
    "            y_pred = models[key].predict(X_val_scaled)\n",
    "            mse = mean_squared_error(y_val, y_pred)\n",
    "            mae = mean_absolute_error(y_val, y_pred)\n",
    "            r2 = r2_score(y_val, y_pred)\n",
    "            performance[key] = {'mse': mse, 'mae': mae, 'r2': r2}\n",
    "            print(f\"Performance of {clfName} with {scaler_type} scaler and PCA={use_pca}: MSE={mse}, MAE={mae}, R2={r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of SVR with standard scaler and PCA=True: MSE=89.76062511715364, MAE=6.356003295928954, R2=0.19634706409390346\n",
      "Performance of SVR with standard scaler and PCA=False: MSE=78.95616094014734, MAE=5.841599492634313, R2=0.2930825686139583\n",
      "Performance of SVR with minmax scaler and PCA=True: MSE=78.74912613182043, MAE=5.808798686392694, R2=0.29493621135908754\n",
      "Performance of SVR with minmax scaler and PCA=False: MSE=87.8756541494357, MAE=6.264030260698204, R2=0.21322375641113178\n"
     ]
    }
   ],
   "source": [
    "# Valutazione modelli tradizionali\n",
    "for clfName in ['SVR']:\n",
    "    for scaler_type in preprocessing_options[clfName]['scaler_type']:\n",
    "        for use_pca in preprocessing_options[clfName]['use_pca']:\n",
    "            key = f\"{clfName}_{scaler_type}_{'PCA' if use_pca else 'NoPCA'}\"\n",
    "            X_val_scaled = scalers[key].transform(X_val)\n",
    "            if pcas[key]:\n",
    "                X_val_scaled = pcas[key].transform(X_val_scaled)\n",
    "            y_pred = models[key].predict(X_val_scaled)\n",
    "            mse = mean_squared_error(y_val, y_pred)\n",
    "            mae = mean_absolute_error(y_val, y_pred)\n",
    "            r2 = r2_score(y_val, y_pred)\n",
    "            performance[key] = {'mse': mse, 'mae': mae, 'r2': r2}\n",
    "            print(f\"Performance of {clfName} with {scaler_type} scaler and PCA={use_pca}: MSE={mse}, MAE={mae}, R2={r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation FFNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of FFNN with standard scaler and PCA=True: MSE=3827140.8363093333, MAE=1956.1712132502987, R2=-34264.50390625\n",
      "Performance of FFNN with standard scaler and PCA=False: MSE=3606756.0399361844, MAE=1897.8917027504365, R2=-32291.3359375\n",
      "Performance of FFNN with minmax scaler and PCA=True: MSE=3965149.11985667, MAE=1991.2404726114385, R2=-35500.12890625\n",
      "Performance of FFNN with minmax scaler and PCA=False: MSE=3461327.940968171, MAE=1860.4338750381446, R2=-30989.271484375\n"
     ]
    }
   ],
   "source": [
    "# Valutazione FFNN\n",
    "for scaler_type in preprocessing_options['FFNN']['scaler_type']:\n",
    "    for use_pca in preprocessing_options['FFNN']['use_pca']:\n",
    "        key = f\"FFNN_{scaler_type}_{'PCA' if use_pca else 'NoPCA'}\"\n",
    "        X_val_scaled = scalers[key].transform(X_val)\n",
    "        if pcas[key]:\n",
    "            X_val_scaled = pcas[key].transform(X_val_scaled)\n",
    "        X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32)\n",
    "        y_pred_tensor = models[key](X_val_tensor).detach().numpy().squeeze()\n",
    "        mse = mean_squared_error(y_val, y_pred_tensor)\n",
    "        mae = mean_absolute_error(y_val, y_pred_tensor)\n",
    "        r2 = r2_score(y_val, y_pred_tensor)\n",
    "        performance[key] = {'mse': mse, 'mae': mae, 'r2': r2}\n",
    "        print(f\"Performance of FFNN with {scaler_type} scaler and PCA={use_pca}: MSE={mse}, MAE={mae}, R2={r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation TabNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valutazione TabNet\n",
    "for scaler_type in preprocessing_options['TabNet']['scaler_type']:\n",
    "    key = f\"TabNet_{scaler_type}_NoPCA\"\n",
    "    X_val_scaled = scalers[key].transform(X_val)\n",
    "    y_pred = models[key].predict(X_val_scaled)\n",
    "    mse = mean_squared_error(y_val, y_pred)\n",
    "    mae = mean_absolute_error(y_val, y_pred)\n",
    "    r2 = r2_score(y_val, y_pred)\n",
    "    performance[key] = {'mse': mse, 'mae': mae, 'r2': r2}\n",
    "    print(f\"Performance of TabNet with {scaler_type} scaler: MSE={mse}, MAE={mae}, R2={r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation TabTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valutazione TabTransformer\n",
    "for scaler_type in preprocessing_options['TabTransformer']['scaler_type']:\n",
    "    key = f\"TabTransformer_{scaler_type}_NoPCA\"\n",
    "    y_pred = models[key].predict(df_val).squeeze()\n",
    "    mse = mean_squared_error(y_val, y_pred)\n",
    "    mae = mean_absolute_error(y_val, y_pred)\n",
    "    r2 = r2_score(y_val, y_pred)\n",
    "    performance[key] = {'mse': mse, 'mae': mae, 'r2': r2}\n",
    "    print(f\"Performance of TabTransformer with {scaler_type} scaler: MSE={mse}, MAE={mae}, R2={r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvataggio delle performance in un file CSV\n",
    "performance_df = pd.DataFrame(performance_dict)\n",
    "performance_df.to_csv('performance_results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
